{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e412620a",
   "metadata": {},
   "source": [
    "# RoboPianist Video Generation\n",
    "\n",
    "This notebook is designed to generate videos from pre-trained RoboPianist models. It's particularly useful for creating videos locally after training on HPC clusters where video recording was disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e898989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from dataclasses import dataclass, asdict\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML, clear_output, display\n",
    "from base64 import b64encode\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Setup paths\n",
    "base_dir = os.getcwd()  \n",
    "robopianist_dir = os.path.join(base_dir, \"robopianist\")\n",
    "os.chdir(robopianist_dir)\n",
    "if robopianist_dir not in sys.path:\n",
    "    sys.path.insert(0, robopianist_dir)\n",
    "\n",
    "# RoboPianist imports\n",
    "from robopianist import suite, music\n",
    "import dm_env_wrappers as wrappers\n",
    "import robopianist.wrappers as robopianist_wrappers\n",
    "import dm_env\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ff253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoGenerationArgs:\n",
    "    # Environment settings (should match training)\n",
    "    environment_name: str = \"RoboPianist-debug-TwinkleTwinkleRousseau-v0\"\n",
    "    seed: int = 42\n",
    "    control_timestep: float = 0.05\n",
    "    n_steps_lookahead: int = 10\n",
    "    trim_silence: bool = True\n",
    "    gravity_compensation: bool = True\n",
    "    reduced_action_space: bool = True\n",
    "    primitive_fingertip_collisions: bool = True\n",
    "    action_reward_observation: bool = True\n",
    "    \n",
    "    # Video generation settings\n",
    "    hpc_mode: bool = False  # MUST be False for video generation\n",
    "    eval_episodes: int = 3  # Number of episodes to record per model\n",
    "    deterministic: bool = True  # Use deterministic policy for reproducible videos\n",
    "    \n",
    "    # Paths\n",
    "    models_dir: str = \"/tmp/robopianist\"\n",
    "    output_dir: str = \"results/videos\" \n",
    "    \n",
    "    # Video settings\n",
    "    camera_id: str = \"piano/back\" \n",
    "    video_height: int = 480\n",
    "    video_width: int = 640\n",
    "    record_every: int = 1 \n",
    "    \n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create configuration\n",
    "video_args = VideoGenerationArgs()\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_path = Path(video_args.output_dir)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Videos will be saved to: {output_path.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b8dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        dims = (input_dim,) + hidden_dims + (output_dim,)\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            layers.append(nn.GELU())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.backbone = MLP(obs_dim, 2 * action_dim, hidden_dims)\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        outputs = self.backbone(obs)\n",
    "        means, log_stds = torch.chunk(outputs, 2, dim=-1)\n",
    "        log_stds = torch.clamp(log_stds, -20, 2)\n",
    "        return means, log_stds\n",
    "    \n",
    "    def sample(self, obs, deterministic=False):\n",
    "        means, log_stds = self.forward(obs)\n",
    "        stds = torch.exp(log_stds)\n",
    "        \n",
    "        if deterministic:\n",
    "            actions = torch.tanh(means)\n",
    "            log_probs = None\n",
    "        else:\n",
    "            dist = Normal(means, stds)\n",
    "            samples = dist.rsample()\n",
    "            actions = torch.tanh(samples)\n",
    "            \n",
    "            log_probs = dist.log_prob(samples)\n",
    "            log_probs -= torch.log(1 - actions.pow(2) + 1e-6)\n",
    "            log_probs = log_probs.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "        return actions, log_probs\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.network = MLP(obs_dim + action_dim, 1, hidden_dims)\n",
    "    \n",
    "    def forward(self, obs, action):\n",
    "        x = torch.cat([obs, action], dim=-1)\n",
    "        return self.network(x)\n",
    "\n",
    "class TwinQNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.q1 = QNetwork(obs_dim, action_dim, hidden_dims)\n",
    "        self.q2 = QNetwork(obs_dim, action_dim, hidden_dims)\n",
    "    \n",
    "    def forward(self, obs, action):\n",
    "        return self.q1(obs, action), self.q2(obs, action)\n",
    "\n",
    "class SAC:\n",
    "    def __init__(self, obs_dim: int, action_dim: int, args):\n",
    "        self.device = torch.device(args.device)\n",
    "        self.discount = getattr(args, 'discount', 0.99)\n",
    "        self.tau = getattr(args, 'tau', 0.005)\n",
    "        self.target_entropy = -action_dim\n",
    "        \n",
    "        # Networks (use default hidden dims if not specified)\n",
    "        hidden_dims = getattr(args, 'hidden_dims', (256, 256, 256))\n",
    "        self.actor = Policy(obs_dim, action_dim, hidden_dims).to(self.device)\n",
    "        self.critic = TwinQNetwork(obs_dim, action_dim, hidden_dims).to(self.device)\n",
    "        self.target_critic = TwinQNetwork(obs_dim, action_dim, hidden_dims).to(self.device)\n",
    "        \n",
    "        self.actor = self.actor.float()\n",
    "        self.critic = self.critic.float()\n",
    "        self.target_critic = self.target_critic.float()\n",
    "        \n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        # Initialize log_alpha\n",
    "        init_temp = getattr(args, 'init_temperature', 1.0)\n",
    "        self.log_alpha = torch.tensor(np.log(init_temp), dtype=torch.float32, requires_grad=True, device=self.device)\n",
    "        \n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self.log_alpha.exp()\n",
    "    \n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.from_numpy(obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "            action, _ = self.actor.sample(obs_tensor, deterministic=deterministic)\n",
    "            return action.cpu().numpy()[0].astype(np.float32)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "        self.target_critic.load_state_dict(checkpoint['target_critic'])\n",
    "        self.log_alpha = checkpoint['log_alpha']\n",
    "        print(f\"Model loaded from {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ac310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(args: VideoGenerationArgs, record_dir: Optional[Path] = None):\n",
    "    \"\"\"Create environment for video generation.\"\"\"\n",
    "    env = suite.load(\n",
    "        environment_name=args.environment_name,\n",
    "        seed=args.seed,\n",
    "        task_kwargs=dict(\n",
    "            n_steps_lookahead=args.n_steps_lookahead,\n",
    "            trim_silence=args.trim_silence,\n",
    "            gravity_compensation=args.gravity_compensation,\n",
    "            reduced_action_space=args.reduced_action_space,\n",
    "            control_timestep=args.control_timestep,\n",
    "            primitive_fingertip_collisions=args.primitive_fingertip_collisions,\n",
    "            change_color_on_activation=True,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    if record_dir is not None:\n",
    "        env = robopianist_wrappers.PianoSoundVideoWrapper(\n",
    "            environment=env,\n",
    "            record_dir=record_dir,\n",
    "            record_every=args.record_every,\n",
    "            camera_id=args.camera_id,\n",
    "            height=args.video_height,\n",
    "            width=args.video_width,\n",
    "        )\n",
    "    \n",
    "    env = wrappers.EpisodeStatisticsWrapper(environment=env, deque_size=1)\n",
    "    env = robopianist_wrappers.MidiEvaluationWrapper(environment=env, deque_size=1)\n",
    "    \n",
    "    if args.action_reward_observation:\n",
    "        env = wrappers.ObservationActionRewardWrapper(env)\n",
    "    \n",
    "    env = wrappers.ConcatObservationWrapper(env)\n",
    "    env = wrappers.CanonicalSpecWrapper(env, clip=True)\n",
    "    env = wrappers.SinglePrecisionWrapper(env)\n",
    "    env = wrappers.DmControlWrapper(env)\n",
    "    \n",
    "    return env\n",
    "\n",
    "# Test environment setup\n",
    "print(\"Setting up environment...\")\n",
    "test_env = get_env(video_args)\n",
    "test_timestep = test_env.reset()\n",
    "\n",
    "obs_dim = test_timestep.observation.shape[0]\n",
    "action_dim = test_env.action_spec().shape[0]\n",
    "\n",
    "print(f\"Environment setup complete!\")\n",
    "print(f\"Observation dimension: {obs_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f86621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_model_files(directory: str, pattern: str = \"*.pt\") -> List[Path]:\n",
    "    search_path = Path(directory)\n",
    "    if not search_path.exists():\n",
    "        print(f\"Directory {directory} does not exist!\")\n",
    "        return []\n",
    "    \n",
    "    model_files = []\n",
    "    for file_path in search_path.rglob(pattern):\n",
    "        model_files.append(file_path)\n",
    "    \n",
    "    # Sort by modification time (newest first)\n",
    "    model_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    return model_files\n",
    "\n",
    "def extract_model_info(model_path: Path) -> Dict[str, any]:\n",
    "    filename = model_path.stem\n",
    "    info = {\n",
    "        'path': model_path,\n",
    "        'filename': filename,\n",
    "        'step': None,\n",
    "        'eval_num': None,\n",
    "        'is_final': False\n",
    "    }\n",
    "    \n",
    "    if 'final_model' in filename:\n",
    "        info['is_final'] = True\n",
    "    elif 'model_step_' in filename:\n",
    "        parts = filename.split('_')\n",
    "        for i, part in enumerate(parts):\n",
    "            if part == 'step' and i + 1 < len(parts):\n",
    "                try:\n",
    "                    info['step'] = int(parts[i + 1])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            elif part == 'eval' and i + 1 < len(parts):\n",
    "                try:\n",
    "                    info['eval_num'] = int(parts[i + 1])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    \n",
    "    return info\n",
    "\n",
    "def play_video(filename: str, width: int = 640, height: int = 480):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Video file not found: {filename}\")\n",
    "        return None\n",
    "    \n",
    "    mp4 = open(filename, \"rb\").read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    \n",
    "    return HTML(f\"\"\"\n",
    "    <video controls width=\"{width}\" height=\"{height}\">\n",
    "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\")\n",
    "\n",
    "def save_evaluation_results(results: Dict, output_file: Path):\n",
    "    json_results = {}\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            json_results[key] = value.tolist()\n",
    "        elif isinstance(value, (np.integer, np.floating)):\n",
    "            json_results[key] = float(value)\n",
    "        else:\n",
    "            json_results[key] = value\n",
    "    \n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(json_results, f, indent=2)\n",
    "    print(f\"Results saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df00713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_videos_for_model(model_path: Path, args: VideoGenerationArgs, \n",
    "                            output_subdir: str = None) -> Dict[str, any]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing model: {model_path.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create output directory for this model\n",
    "    if output_subdir:\n",
    "        video_output_dir = Path(args.output_dir) / output_subdir\n",
    "    else:\n",
    "        model_name = model_path.stem\n",
    "        video_output_dir = Path(args.output_dir) / model_name\n",
    "    \n",
    "    video_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create environment with video recording\n",
    "    env = get_env(args, record_dir=video_output_dir)\n",
    "    \n",
    "    # Initialize agent and load model\n",
    "    agent = SAC(obs_dim, action_dim, args)\n",
    "    try:\n",
    "        agent.load(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_path}: {e}\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Generating {args.eval_episodes} videos...\")\n",
    "    \n",
    "    eval_returns = []\n",
    "    eval_f1s = []\n",
    "    eval_precisions = []\n",
    "    eval_recalls = []\n",
    "    video_files = []\n",
    "    \n",
    "    for episode in range(args.eval_episodes):\n",
    "        print(f\"Recording episode {episode + 1}/{args.eval_episodes}...\")\n",
    "        \n",
    "        timestep = env.reset()\n",
    "        eval_return = 0.0\n",
    "        step_count = 0\n",
    "        \n",
    "        while not timestep.last():\n",
    "            action = agent.select_action(timestep.observation, deterministic=args.deterministic)\n",
    "            timestep = env.step(action)\n",
    "            eval_return += timestep.reward\n",
    "            step_count += 1\n",
    "        \n",
    "        eval_returns.append(eval_return)\n",
    "        \n",
    "        # Get musical metrics\n",
    "        try:\n",
    "            musical_metrics = env.get_musical_metrics()\n",
    "            eval_f1s.append(musical_metrics['f1'])\n",
    "            eval_precisions.append(musical_metrics['precision'])\n",
    "            eval_recalls.append(musical_metrics['recall'])\n",
    "        except (AttributeError, ValueError) as e:\n",
    "            print(f\"Warning: Could not get musical metrics: {e}\")\n",
    "            eval_f1s.append(0.0)\n",
    "            eval_precisions.append(0.0)\n",
    "            eval_recalls.append(0.0)\n",
    "        \n",
    "        print(f\"  Episode {episode + 1}: Return = {eval_return:.2f}, F1 = {eval_f1s[-1]:.4f}, Steps = {step_count}\")\n",
    "        \n",
    "        # Find the generated video file\n",
    "        video_pattern = video_output_dir / f\"*episode_{episode}*.mp4\"\n",
    "        videos = list(video_output_dir.glob(\"*.mp4\"))\n",
    "        if videos:\n",
    "            # Get the most recently created video\n",
    "            latest_video = max(videos, key=lambda x: x.stat().st_ctime)\n",
    "            video_files.append(latest_video)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    results = {\n",
    "        'model_path': str(model_path),\n",
    "        'model_name': model_path.stem,\n",
    "        'video_output_dir': str(video_output_dir),\n",
    "        'eval_episodes': args.eval_episodes,\n",
    "        'returns': eval_returns,\n",
    "        'f1_scores': eval_f1s,\n",
    "        'precision_scores': eval_precisions,\n",
    "        'recall_scores': eval_recalls,\n",
    "        'mean_return': np.mean(eval_returns),\n",
    "        'std_return': np.std(eval_returns),\n",
    "        'mean_f1': np.mean(eval_f1s),\n",
    "        'std_f1': np.std(eval_f1s),\n",
    "        'mean_precision': np.mean(eval_precisions),\n",
    "        'mean_recall': np.mean(eval_recalls),\n",
    "        'video_files': [str(v) for v in video_files],\n",
    "        'total_videos': len(video_files)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults for {model_path.name}:\")\n",
    "    print(f\"  Mean Return: {results['mean_return']:.2f} ± {results['std_return']:.2f}\")\n",
    "    print(f\"  Mean F1: {results['mean_f1']:.4f} ± {results['std_f1']:.4f}\")\n",
    "    print(f\"  Videos generated: {results['total_videos']}\")\n",
    "    print(f\"  Videos saved to: {video_output_dir}\")\n",
    "    \n",
    "    # Save results to JSON\n",
    "    results_file = video_output_dir / \"evaluation_results.json\"\n",
    "    save_evaluation_results(results, results_file)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def batch_generate_videos(models_dir: str, args: VideoGenerationArgs, \n",
    "                         max_models: int = None, model_filter: str = None) -> List[Dict]:\n",
    "    print(f\"Searching for models in: {models_dir}\")\n",
    "    \n",
    "    # Find all model files\n",
    "    model_files = find_model_files(models_dir)\n",
    "    \n",
    "    if not model_files:\n",
    "        print(\"No model files found!\")\n",
    "        return []\n",
    "    \n",
    "    # Apply filter if specified\n",
    "    if model_filter:\n",
    "        model_files = [f for f in model_files if model_filter in f.name]\n",
    "        print(f\"Filtered to {len(model_files)} models matching '{model_filter}'\")\n",
    "    \n",
    "    # Limit number of models if specified\n",
    "    if max_models:\n",
    "        model_files = model_files[:max_models]\n",
    "        print(f\"Processing first {len(model_files)} models\")\n",
    "    \n",
    "    print(f\"Found {len(model_files)} model files to process\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for i, model_path in enumerate(model_files):\n",
    "        try:\n",
    "            model_info = extract_model_info(model_path)\n",
    "            print(f\"\\nProcessing model {i+1}/{len(model_files)}: {model_path.name}\")\n",
    "            \n",
    "            # Create organized subdirectory name\n",
    "            if model_info['is_final']:\n",
    "                subdir = f\"final_model_{model_path.parent.name}\"\n",
    "            elif model_info['step'] is not None:\n",
    "                subdir = f\"step_{model_info['step']:06d}_eval_{model_info['eval_num']}\"\n",
    "            else:\n",
    "                subdir = f\"model_{i+1:03d}_{model_path.stem}\"\n",
    "            \n",
    "            results = generate_videos_for_model(model_path, args, subdir)\n",
    "            if results:\n",
    "                results['model_info'] = model_info\n",
    "                all_results.append(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Batch processing complete!\")\n",
    "    print(f\"Successfully processed {len(all_results)}/{len(model_files)} models\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bf25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Searching for available models...\")\n",
    "available_models = find_model_files(video_args.models_dir)\n",
    "\n",
    "if available_models:\n",
    "    print(f\"Found {len(available_models)} model files:\")\n",
    "    print(\"\\nAvailable models:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, model_path in enumerate(available_models[:10]):  # Show first 10\n",
    "        model_info = extract_model_info(model_path)\n",
    "        size_mb = model_path.stat().st_size / (1024 * 1024)\n",
    "        \n",
    "        if model_info['is_final']:\n",
    "            model_type = \"Final Model\"\n",
    "        elif model_info['step'] is not None:\n",
    "            model_type = f\"Step {model_info['step']}, Eval {model_info['eval_num']}\"\n",
    "        else:\n",
    "            model_type = \"Unknown\"\n",
    "        \n",
    "        print(f\"{i+1:2d}. {model_path.name}\")\n",
    "        print(f\"    Path: {model_path}\")\n",
    "        print(f\"    Type: {model_type}\")\n",
    "        print(f\"    Size: {size_mb:.1f} MB\")\n",
    "        print(f\"    Modified: {time.ctime(model_path.stat().st_mtime)}\")\n",
    "        print()\n",
    "    \n",
    "    if len(available_models) > 10:\n",
    "        print(f\"... and {len(available_models) - 10} more models\")\n",
    "        \n",
    "else:\n",
    "    print(f\"No model files found in {video_args.models_dir}\")\n",
    "    print(\"Please check the models_dir path in the configuration above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74639ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Generate videos for a single model (modify the path as needed)\n",
    "if available_models:\n",
    "    # Use the most recent model by default\n",
    "    selected_model = available_models[0]\n",
    "    \n",
    "    print(f\"Generating videos for: {selected_model.name}\")\n",
    "    \n",
    "    single_results = generate_videos_for_model(\n",
    "        model_path=selected_model,\n",
    "        args=video_args,\n",
    "        output_subdir=\"single_model_demo\"\n",
    "    )\n",
    "    \n",
    "    # Display the results\n",
    "    if single_results and single_results['video_files']:\n",
    "        print(f\"\\nDisplaying first video:\")\n",
    "        first_video = single_results['video_files'][0]\n",
    "        display(play_video(first_video))\n",
    "else:\n",
    "    print(\"No models available for video generation.\")\n",
    "\n",
    "# Example 2: Batch process multiple models (uncomment to run)\n",
    "# \n",
    "# # Process evaluation models only (models with \"eval\" in the name)\n",
    "# batch_results = batch_generate_videos(\n",
    "#     models_dir=video_args.models_dir,\n",
    "#     args=video_args,\n",
    "#     max_models=5,  # Limit to first 5 models\n",
    "#     model_filter=\"eval\"  # Only process evaluation models\n",
    "# )\n",
    "# \n",
    "# # Display summary\n",
    "# if batch_results:\n",
    "#     print(\"\\nBatch Processing Summary:\")\n",
    "#     print(\"=\" * 60)\n",
    "#     for result in batch_results:\n",
    "#         print(f\"Model: {result['model_name']}\")\n",
    "#         print(f\"  F1 Score: {result['mean_f1']:.4f} ± {result['std_f1']:.4f}\")\n",
    "#         print(f\"  Return: {result['mean_return']:.2f} ± {result['std_return']:.2f}\")\n",
    "#         print(f\"  Videos: {result['total_videos']}\")\n",
    "#         print()\n",
    "\n",
    "# Example 3: Custom model selection\n",
    "# Uncomment and modify this section to process specific models\n",
    "\n",
    "# # Define specific models to process\n",
    "# custom_models = [\n",
    "#     \"final_model.pt\",\n",
    "#     \"model_step_10000_eval_1.pt\", \n",
    "#     \"model_step_20000_eval_2.pt\",\n",
    "#     \"model_step_30000_eval_3.pt\"\n",
    "# ]\n",
    "# \n",
    "# custom_results = []\n",
    "# models_dir_path = Path(video_args.models_dir)\n",
    "# \n",
    "# for model_name in custom_models:\n",
    "#     model_files = list(models_dir_path.rglob(model_name))\n",
    "#     \n",
    "#     if model_files:\n",
    "#         model_path = model_files[0]  # Use first match\n",
    "#         print(f\"Processing: {model_name}\")\n",
    "#         \n",
    "#         results = generate_videos_for_model(\n",
    "#             model_path=model_path,\n",
    "#             args=video_args\n",
    "#         )\n",
    "#         \n",
    "#         if results:\n",
    "#             custom_results.append(results)\n",
    "#     else:\n",
    "#         print(f\"Model not found: {model_name}\")\n",
    "# \n",
    "# print(f\"Processed {len(custom_results)} custom models\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab1d68fa",
   "metadata": {},
   "source": [
    "# RoboPianist Training and Evaluation on ‚ÄúGolden Hour‚Äù Snippet (Bars¬†7¬†&¬†9) by JVKE\n",
    "\n",
    "This notebook tests RoboPianist‚Äôs ability to generalize beyond its classical only training set (ROBOPIANIST‚ÄëREPERTOIRE‚Äë150) by evaluating it on the pop song from bars¬†7 and¬†9 of JVKE‚Äôs ‚ÄúGolden¬†Hour.‚Äù We‚Äôve ported the original kevinzakka/robopianist-rl (https://github.com/kevinzakka/robopianist-rl.git) training pipeline from JAX to PyTorch and adapted the structure of the official‚ÄØtutorial.ipynb (https://github.com/google-research/robopianist/blob/main/tutorial.ipynb) for easy to use and full reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98d4a6",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "1. Follow the [RoboPianist installation instructions](https://github.com/google-research/robopianist?tab=readme-ov-file#installation) to set up the pianist conda environmen.\n",
    "2. Intall the following packages in the pianist conda environment:\n",
    "   ```bash\n",
    "   conda activate pianist\n",
    "   conda install \"numpy<2.0.0\"\n",
    "   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "   conda install -c conda-forge matplotlib tqdm wandb tyro\n",
    "   pip install wandb\n",
    "   ```\n",
    "3. To verify that everything is installed and downloaded, please run once the [tutorial.ipynb](robopianist/tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62a6e3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import time\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from tqdm import tqdm\n",
    "import tyro\n",
    "from IPython.display import HTML, clear_output\n",
    "from base64 import b64encode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_dir = os.getcwd()  \n",
    "robopianist_dir = os.path.join(base_dir, \"robopianist\")\n",
    "os.chdir(robopianist_dir)\n",
    "if robopianist_dir not in sys.path:\n",
    "    sys.path.insert(0, robopianist_dir)\n",
    "\n",
    "# RoboPianist imports\n",
    "from robopianist import suite, music\n",
    "import dm_env_wrappers as wrappers\n",
    "import robopianist.wrappers as robopianist_wrappers\n",
    "import dm_env\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f8cdd",
   "metadata": {},
   "source": [
    "## Prepare Golden Hour for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingArgs:\n",
    "    hpc_mode: bool = True  # Set to False for local runs with video recording\n",
    "    \n",
    "    # Environment settings\n",
    "    environment_name: str = \"RoboPianist-debug-TwinkleTwinkleRousseau-v0\"\n",
    "    seed: int = 42\n",
    "    control_timestep: float = 0.05\n",
    "    n_steps_lookahead: int = 10\n",
    "    trim_silence: bool = True\n",
    "    gravity_compensation: bool = True\n",
    "    reduced_action_space: bool = True\n",
    "    primitive_fingertip_collisions: bool = True\n",
    "    action_reward_observation: bool = True\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    max_steps: int = 5000000 \n",
    "    warmstart_steps: int = 5000\n",
    "    batch_size: int = 256\n",
    "    replay_capacity: int = 5000000\n",
    "\n",
    "    # SAC hyperparameters\n",
    "    actor_lr: float = 3e-4\n",
    "    critic_lr: float = 3e-4\n",
    "    temp_lr: float = 3e-4\n",
    "    hidden_dims: Tuple[int, ...] = (256, 256, 256)\n",
    "    discount: float = 0.99\n",
    "    tau: float = 0.005\n",
    "    init_temperature: float = 1.0\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    log_interval: int = 1000\n",
    "    eval_interval: int = 10000\n",
    "    eval_episodes: int = 1  # Back to 1 to avoid identical episodes (can set to 3+ if needed)\n",
    "    tqdm_bar: bool = True\n",
    "    \n",
    "    # Paths and wandb\n",
    "    base_dir = os.getcwd()\n",
    "    parent_dir = os.path.dirname(base_dir)\n",
    "    root_dir: str = str(os.path.join(parent_dir, \"runs\"))\n",
    "    project: str = \"robopianist-pytorch\"\n",
    "    mode: str = \"disabled\"  # Set to \"online\" to enable wandb logging\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create training configuration\n",
    "args = TrainingArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85675a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from note_seq.protobuf import music_pb2\n",
    "from robopianist.music import midi_file\n",
    "\n",
    "def parse_golden_hour_fingering_txt(file_path: str) -> midi_file.MidiFile:\n",
    "    \n",
    "    # Note name to MIDI number mapping with support for sharps/flats\n",
    "    def spelled_pitch_to_midi(spelled_pitch: str) -> int:\n",
    "        # Parse note name with possible multiple sharps/flats\n",
    "        match = re.match(r'([A-G])([#b]*)(\\d+)', spelled_pitch)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Invalid spelled pitch: {spelled_pitch}\")\n",
    "        \n",
    "        note_letter, accidentals, octave = match.groups()\n",
    "        octave = int(octave)\n",
    "        \n",
    "        # Base MIDI numbers for note letters (C4 = 60)\n",
    "        note_base = {'C': 0, 'D': 2, 'E': 4, 'F': 5, 'G': 7, 'A': 9, 'B': 11}\n",
    "        \n",
    "        # Calculate MIDI number\n",
    "        midi_number = (octave + 1) * 12 + note_base[note_letter]\n",
    "        \n",
    "        # Apply accidentals\n",
    "        for accidental in accidentals:\n",
    "            if accidental == '#':\n",
    "                midi_number += 1\n",
    "            elif accidental == 'b':\n",
    "                midi_number -= 1\n",
    "        \n",
    "        return midi_number\n",
    "    \n",
    "    def parse_finger_number(finger_str: str, channel: int) -> int:\n",
    "        # Handle finger substitutions (e.g., \"3_1\")\n",
    "        if '_' in finger_str:\n",
    "            # For substitutions, use the first finger mentioned\n",
    "            finger_str = finger_str.split('_')[0]\n",
    "        \n",
    "        finger_num = int(finger_str)\n",
    "        \n",
    "        if channel == 0:  # Right hand\n",
    "            # Right hand: 1-5 maps to RoboPianist fingers 0-4\n",
    "            return finger_num - 1\n",
    "        else:  # Left hand (channel == 1)\n",
    "            # Left hand: -1 to -5 becomes RoboPianist fingers 5-9\n",
    "            if finger_num > 0:\n",
    "                # If positive number for left hand, convert to negative\n",
    "                finger_num = -finger_num\n",
    "            return 5 + abs(finger_num) - 1 \n",
    "    \n",
    "    notes_data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    print(f\"Found {len(lines)} lines\")\n",
    "    \n",
    "    for line_num, line in enumerate(lines, 1):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if line.startswith('//') or line.startswith('#') or not line:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 8:\n",
    "                note_id = int(parts[0])\n",
    "                onset_time = float(parts[1])\n",
    "                offset_time = float(parts[2])\n",
    "                spelled_pitch = parts[3]\n",
    "                onset_velocity = int(parts[4])\n",
    "                offset_velocity = int(parts[5])\n",
    "                channel = int(parts[6]) \n",
    "                finger_number_str = parts[7]\n",
    "                \n",
    "                midi_number = spelled_pitch_to_midi(spelled_pitch)\n",
    "                \n",
    "                robopianist_finger = parse_finger_number(finger_number_str, channel)\n",
    "                \n",
    "                notes_data.append({\n",
    "                    'note_id': note_id,\n",
    "                    'onset_time': onset_time,\n",
    "                    'offset_time': offset_time,\n",
    "                    'spelled_pitch': spelled_pitch,\n",
    "                    'midi_number': midi_number,\n",
    "                    'onset_velocity': onset_velocity,\n",
    "                    'offset_velocity': offset_velocity,\n",
    "                    'channel': channel,\n",
    "                    'finger_number_str': finger_number_str,\n",
    "                    'robopianist_finger': robopianist_finger\n",
    "                })\n",
    "                \n",
    "        except (ValueError, IndexError) as e:\n",
    "            print(f\"Could not parse line {line_num}: {line} (Error: {e})\")\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    # Create MIDI sequence\n",
    "    seq = music_pb2.NoteSequence()\n",
    "    \n",
    "    # Add metadata\n",
    "    seq.sequence_metadata.title = \"Golden Hour (Bars 7 & 9)\"\n",
    "    seq.sequence_metadata.artist = \"JVKE\"\n",
    "    \n",
    "    # Add all notes to the sequence\n",
    "    for note_data in notes_data:\n",
    "        seq.notes.add(\n",
    "            pitch=note_data['midi_number'],\n",
    "            start_time=note_data['onset_time'],\n",
    "            end_time=note_data['offset_time'],\n",
    "            velocity=note_data['onset_velocity'],\n",
    "            part=note_data['robopianist_finger']  # RoboPianist fingering field\n",
    "        )\n",
    "    \n",
    "    # Set total time to the last note's end time\n",
    "    if notes_data:\n",
    "        seq.total_time = max(note['offset_time'] for note in notes_data)\n",
    "        seq.tempos.add(qpm=120)  # Default tempo\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    print(f\"\\nAnalysis:\")\n",
    "    print(f\"  Duration: {seq.total_time:.2f} seconds\")\n",
    "    print(f\"  Total notes: {len(notes_data)}\")\n",
    "    \n",
    "    # Channel analysis (hand separation)\n",
    "    rh_notes = [n for n in notes_data if n['channel'] == 0]\n",
    "    lh_notes = [n for n in notes_data if n['channel'] == 1]\n",
    "    print(f\"  Right hand notes (channel 0): {len(rh_notes)}\")\n",
    "    print(f\"  Left hand notes (channel 1): {len(lh_notes)}\")\n",
    "    \n",
    "    # Fingering analysis\n",
    "    robopianist_fingers = [note['robopianist_finger'] for note in notes_data]\n",
    "    unique_fingers = sorted(set(robopianist_fingers))\n",
    "    print(f\"  RoboPianist fingers used: {unique_fingers}\")\n",
    "    \n",
    "    # Velocity analysis\n",
    "    velocities = [note['onset_velocity'] for note in notes_data]\n",
    "    print(f\"  Velocity range: {min(velocities)}-{max(velocities)}\")\n",
    "    \n",
    "    # Show first few notes for verification\n",
    "    print(f\"\\nüéº First 5 notes:\")\n",
    "    for i, note in enumerate(notes_data[:5]):\n",
    "        hand = \"RH\" if note['channel'] == 0 else \"LH\"\n",
    "        finger_display = f\"finger {note['robopianist_finger']}\"\n",
    "        print(f\"  {i+1}. {note['spelled_pitch']} (MIDI {note['midi_number']}) \"\n",
    "              f\"t={note['onset_time']:.3f}-{note['offset_time']:.3f}s \"\n",
    "              f\"{hand} {finger_display} (raw: {note['finger_number_str']})\")\n",
    "    \n",
    "    return midi_file.MidiFile(seq=seq)\n",
    "\n",
    "# Parse the Golden Hour file\n",
    "golden_hour_file = \"/dss/dsstbyfs02/pn52ru/pn52ru-dss-0000/di97jur/robopianist_evaluation/golden_hour_fingering.txt\"\n",
    "golden_hour_midi = parse_golden_hour_fingering_txt(golden_hour_file)\n",
    "\n",
    "print(f\"\\nüéµ Created Golden Hour MIDI:\")\n",
    "print(f\"  Title: {golden_hour_midi.title}\")\n",
    "print(f\"  Artist: {golden_hour_midi.artist}\")\n",
    "print(f\"  Duration: {golden_hour_midi.duration:.2f} seconds\")\n",
    "print(f\"  Number of notes: {golden_hour_midi.n_notes}\")\n",
    "print(f\"  Has fingering: {golden_hour_midi.has_fingering()}\")\n",
    "\n",
    "# Test that it can be used for training\n",
    "def create_golden_hour_from_file() -> midi_file.MidiFile:\n",
    "    return parse_golden_hour_fingering_txt(golden_hour_file)\n",
    "\n",
    "def create_golden_hour_from_file() -> midi_file.MidiFile:\n",
    "    file_path = \"/dss/dsstbyfs02/pn52ru/pn52ru-dss-0000/di97jur/robopianist_evaluation/golden_hour_fingering.txt\"\n",
    "    return parse_golden_hour_fingering_txt(file_path)\n",
    "\n",
    "def get_golden_hour_env(args: TrainingArgs, record_dir: Optional[Path] = None): \n",
    "    # Create custom MIDI\n",
    "    custom_midi = create_golden_hour_from_file()\n",
    "    \n",
    "    # Create environment with custom MIDI\n",
    "    # Note: We need to pass the MIDI directly to the task, not through suite.load\n",
    "    from robopianist.suite.tasks import piano_with_shadow_hands\n",
    "    from dm_control import composer\n",
    "    from robopianist.suite import composer_utils\n",
    "    \n",
    "    # Create the task with our custom MIDI\n",
    "    task = piano_with_shadow_hands.PianoWithShadowHands(\n",
    "        midi=custom_midi,\n",
    "        n_steps_lookahead=args.n_steps_lookahead,\n",
    "        trim_silence=args.trim_silence,\n",
    "        gravity_compensation=args.gravity_compensation,\n",
    "        reduced_action_space=args.reduced_action_space,\n",
    "        control_timestep=args.control_timestep,\n",
    "        primitive_fingertip_collisions=args.primitive_fingertip_collisions,\n",
    "        change_color_on_activation=True,\n",
    "    )\n",
    "    \n",
    "    # Create environment\n",
    "    env = composer_utils.Environment(\n",
    "        task=task,\n",
    "        random_state=args.seed,\n",
    "        strip_singleton_obs_buffer_dim=True,\n",
    "    )\n",
    "    \n",
    "    # Apply the same wrappers as before\n",
    "    if record_dir is not None and not args.hpc_mode:\n",
    "        env = robopianist_wrappers.PianoSoundVideoWrapper(\n",
    "            environment=env,\n",
    "            record_dir=record_dir,\n",
    "            record_every=1,\n",
    "            camera_id=\"piano/back\",\n",
    "            height=480,\n",
    "            width=640,\n",
    "        )\n",
    "    \n",
    "    env = wrappers.EpisodeStatisticsWrapper(environment=env, deque_size=1)\n",
    "    env = robopianist_wrappers.MidiEvaluationWrapper(environment=env, deque_size=1)\n",
    "    \n",
    "    if args.action_reward_observation:\n",
    "        env = wrappers.ObservationActionRewardWrapper(env)\n",
    "    \n",
    "    env = wrappers.ConcatObservationWrapper(env)\n",
    "    env = wrappers.CanonicalSpecWrapper(env, clip=True)\n",
    "    env = wrappers.SinglePrecisionWrapper(env)\n",
    "    env = wrappers.DmControlWrapper(env)\n",
    "    \n",
    "    return env\n",
    "\n",
    "try:\n",
    "    golden_hour_test_env = get_golden_hour_env(args)\n",
    "    test_timestep = golden_hour_test_env.reset()\n",
    "    \n",
    "    print(f\"   Golden Hour environment created successfully!\")\n",
    "    print(f\"   Observation shape: {test_timestep.observation.shape}\")\n",
    "    print(f\"   Action shape: {golden_hour_test_env.action_spec().shape}\")\n",
    "    \n",
    "    # Check if the environment recognizes the custom MIDI\n",
    "    print(f\"   Episode length should match Golden Hour duration\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating Golden Hour environment: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Step 4: Modified training function that uses Golden Hour\n",
    "def train_golden_hour(args: TrainingArgs):\n",
    "    \n",
    "    # Set random seeds\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    print(f\"üéµ Training on Golden Hour (Bars 7 & 9) by JVKE\")\n",
    "    print(f\"Using device: {args.device}\")\n",
    "    print(f\"HPC Mode: {'ENABLED' if args.hpc_mode else 'DISABLED'}\")\n",
    "    \n",
    "    # Create experiment directory\n",
    "    run_name = f\"PyTorch-SAC-GoldenHour-{args.seed}-{int(time.time())}\"\n",
    "    experiment_dir = Path(args.root_dir) / run_name\n",
    "    experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Experiment directory: {experiment_dir}\")\n",
    "    \n",
    "    # Initialize wandb\n",
    "    if args.mode != \"disabled\":\n",
    "        wandb.init(\n",
    "            project=args.project,\n",
    "            config=asdict(args),\n",
    "            mode=args.mode,\n",
    "            name=run_name,\n",
    "        )\n",
    "    \n",
    "    # Create environments with Golden Hour\n",
    "    env = get_golden_hour_env(args)\n",
    "    # Note: eval_env will be created fresh for each evaluation to ensure proper video recording\n",
    "    \n",
    "    # Get dimensions\n",
    "    test_timestep = env.reset()\n",
    "    obs_dim = test_timestep.observation.shape[0]\n",
    "    action_dim = env.action_spec().shape[0]\n",
    "    \n",
    "    # Initialize SAC agent\n",
    "    agent = SAC(obs_dim, action_dim, args)\n",
    "    \n",
    "    # Initialize replay buffer\n",
    "    replay_buffer = ReplayBuffer(obs_dim, action_dim, args.replay_capacity, args.device)\n",
    "    \n",
    "    # Training metrics\n",
    "    training_metrics = {\n",
    "        'steps': [],\n",
    "        'critic_losses': [],\n",
    "        'actor_losses': [],\n",
    "        'alpha_losses': [],\n",
    "        'alphas': [],\n",
    "        'eval_returns': [],\n",
    "        'eval_f1_scores': [], \n",
    "        'eval_precision_scores': [], \n",
    "        'eval_recall_scores': [], \n",
    "        'eval_steps': []\n",
    "    }\n",
    "    \n",
    "    # Add initial timestep\n",
    "    timestep = env.reset()\n",
    "    replay_buffer.add(timestep)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    eval_count = 0 \n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"Starting Golden Hour training for {args.max_steps:,} steps...\")\n",
    "    progress_bar = tqdm(range(1, args.max_steps + 1), desc=\"Golden Hour Training\", unit=\"steps\")\n",
    "    \n",
    "    for step in progress_bar:\n",
    "        # Select action\n",
    "        if step < args.warmstart_steps:\n",
    "            action = env.action_spec().generate_value()\n",
    "        else:\n",
    "            action = agent.select_action(timestep.observation)\n",
    "        \n",
    "        # Step environment\n",
    "        timestep = env.step(action)\n",
    "        replay_buffer.add(timestep, action)\n",
    "        \n",
    "        # Reset if episode ended\n",
    "        if timestep.last():\n",
    "            if args.mode != \"disabled\":\n",
    "                wandb.log(prefix_dict(\"train\", env.get_statistics()), step=step)\n",
    "            timestep = env.reset()\n",
    "            replay_buffer.add(timestep)\n",
    "        \n",
    "        # Update agent\n",
    "        if step >= args.warmstart_steps and len(replay_buffer) >= args.batch_size:\n",
    "            metrics = agent.update(replay_buffer, args.batch_size)\n",
    "            \n",
    "            # Store and update progress bar\n",
    "            if step % args.log_interval == 0:\n",
    "                training_metrics['steps'].append(step)\n",
    "                training_metrics['critic_losses'].append(metrics['critic_loss'])\n",
    "                training_metrics['actor_losses'].append(metrics['actor_loss'])\n",
    "                training_metrics['alpha_losses'].append(metrics['alpha_loss'])\n",
    "                training_metrics['alphas'].append(metrics['alpha'])\n",
    "                \n",
    "                # Update progress bar description with latest metrics\n",
    "                progress_bar.set_postfix({\n",
    "                    'Critic': f\"{metrics['critic_loss']:.3f}\",\n",
    "                    'Actor': f\"{metrics['actor_loss']:.3f}\",\n",
    "                    'Alpha': f\"{metrics['alpha']:.3f}\",\n",
    "                    'Buffer': f\"{len(replay_buffer)}\"\n",
    "                })\n",
    "                \n",
    "                if args.mode != \"disabled\":\n",
    "                    wandb.log(prefix_dict(\"train\", metrics), step=step)\n",
    "        \n",
    "        if step % args.eval_interval == 0:\n",
    "            eval_count += 1 \n",
    "            print(f\"\\nStarting evaluation #{eval_count} at step {step}\")\n",
    "            \n",
    "            eval_returns = []\n",
    "            eval_f1s = []\n",
    "            eval_precisions = []\n",
    "            eval_recalls = []\n",
    "            \n",
    "            actual_eval_episodes = args.eval_episodes\n",
    "            \n",
    "            # Create eval directory for this specific evaluation\n",
    "            eval_video_dir = experiment_dir / \"eval\" / f\"step_{step}\"\n",
    "            eval_video_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            for episode in range(actual_eval_episodes):\n",
    "                print(f\"  üìπ Recording episode {episode + 1}/{actual_eval_episodes}\")\n",
    "                \n",
    "                # Create fresh environment for each episode to ensure proper video recording\n",
    "                episode_eval_env = get_golden_hour_env(args, record_dir=eval_video_dir)\n",
    "                eval_timestep = episode_eval_env.reset()\n",
    "                eval_return = 0.0\n",
    "                \n",
    "                use_deterministic = (episode == 0) if actual_eval_episodes > 1 else True\n",
    "                \n",
    "                while not eval_timestep.last():\n",
    "                    eval_action = agent.select_action(eval_timestep.observation, deterministic=use_deterministic)\n",
    "                    eval_timestep = episode_eval_env.step(eval_action)\n",
    "                    eval_return += eval_timestep.reward\n",
    "                \n",
    "                eval_returns.append(eval_return)\n",
    "                \n",
    "                try:\n",
    "                    musical_metrics = episode_eval_env.get_musical_metrics()\n",
    "                    eval_f1s.append(musical_metrics['f1'])\n",
    "                    eval_precisions.append(musical_metrics['precision'])\n",
    "                    eval_recalls.append(musical_metrics['recall'])\n",
    "                except (AttributeError, ValueError):\n",
    "                    eval_f1s.append(0.0)\n",
    "                    eval_precisions.append(0.0)\n",
    "                    eval_recalls.append(0.0)\n",
    "            \n",
    "            mean_return = np.mean(eval_returns)\n",
    "            mean_f1 = np.mean(eval_f1s)\n",
    "            mean_precision = np.mean(eval_precisions)\n",
    "            mean_recall = np.mean(eval_recalls)\n",
    "            \n",
    "            # Store metrics\n",
    "            training_metrics['eval_returns'].append(mean_return)\n",
    "            training_metrics['eval_f1_scores'].append(mean_f1)\n",
    "            training_metrics['eval_precision_scores'].append(mean_precision)\n",
    "            training_metrics['eval_recall_scores'].append(mean_recall)\n",
    "            training_metrics['eval_steps'].append(step)\n",
    "            \n",
    "            if args.mode != \"disabled\":\n",
    "                eval_metrics = {\n",
    "                    \"eval/return_mean\": mean_return,\n",
    "                    \"eval/f1_mean\": mean_f1,\n",
    "                    \"eval/precision_mean\": mean_precision,\n",
    "                    \"eval/recall_mean\": mean_recall,\n",
    "                }\n",
    "                wandb.log(eval_metrics, step=step)\n",
    "            \n",
    "            eval_info = f\"Golden Hour Eval #{eval_count}: R={mean_return:.1f}, F1={mean_f1:.4f}\"\n",
    "            print(f\"  {eval_info}\")\n",
    "            if not args.hpc_mode:\n",
    "                print(f\"  Videos saved to: {eval_video_dir}\")\n",
    "                \n",
    "            progress_bar.set_postfix({\n",
    "                'Last_Eval': eval_info,\n",
    "                'Buffer': f\"{len(replay_buffer)}\"\n",
    "            })\n",
    "            \n",
    "            # Save model at every evaluation step in HPC mode\n",
    "            if args.hpc_mode:\n",
    "                model_path = experiment_dir / f\"model_step_{step}_eval_{eval_count}.pt\"\n",
    "                agent.save(model_path)\n",
    "                print(f\"Model saved at step {step}: {model_path}\")\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nüéµ Golden Hour training completed in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Total evaluations performed: {eval_count}\")\n",
    "    \n",
    "    # Save final model\n",
    "    model_path = experiment_dir / \"golden_hour_model.pt\"\n",
    "    agent.save(model_path)\n",
    "    print(f\"Golden Hour model saved to: {model_path}\")\n",
    "    \n",
    "    if args.hpc_mode:\n",
    "        print(f\"HPC Mode: {eval_count} evaluation models saved in {experiment_dir}\")\n",
    "        print(\"To generate videos locally later, set hpc_mode=False and reload saved models\")\n",
    "    \n",
    "    if args.mode != \"disabled\":\n",
    "        wandb.finish()\n",
    "    \n",
    "    return agent, training_metrics, experiment_dir\n",
    "\n",
    "print(\"\\n Golden Hour Training Setup Complete!\")\n",
    "print(\"To train on Golden Hour, run:\")\n",
    "print(\"   golden_hour_agent, golden_hour_metrics, golden_hour_exp_dir = train_golden_hour(args)\")\n",
    "print(\"\\n Everything is ready to train RoboPianist on your Golden Hour snippet!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7e154",
   "metadata": {},
   "source": [
    "## Configuration and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa369495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_video(filename: str):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Video file not found: {filename}\")\n",
    "        return None\n",
    "    \n",
    "    mp4 = open(filename, \"rb\").read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    \n",
    "    return HTML(f\"\"\"\n",
    "    <video controls width=\"640\" height=\"480\">\n",
    "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\")\n",
    "\n",
    "def prefix_dict(prefix: str, d: dict) -> dict:\n",
    "    return {f\"{prefix}/{k}\": v for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a593865",
   "metadata": {},
   "source": [
    "## Policy Network in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        dims = (input_dim,) + hidden_dims + (output_dim,)\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            layers.append(nn.GELU())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.backbone = MLP(obs_dim, 2 * action_dim, hidden_dims)\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        outputs = self.backbone(obs)\n",
    "        means, log_stds = torch.chunk(outputs, 2, dim=-1)\n",
    "        log_stds = torch.clamp(log_stds, -20, 2)\n",
    "        return means, log_stds\n",
    "    \n",
    "    def sample(self, obs, deterministic=False):\n",
    "        means, log_stds = self.forward(obs)\n",
    "        stds = torch.exp(log_stds)\n",
    "        \n",
    "        if deterministic:\n",
    "            actions = torch.tanh(means)\n",
    "            log_probs = None\n",
    "        else:\n",
    "            dist = Normal(means, stds)\n",
    "            samples = dist.rsample()\n",
    "            actions = torch.tanh(samples)\n",
    "            \n",
    "            log_probs = dist.log_prob(samples)\n",
    "            log_probs -= torch.log(1 - actions.pow(2) + 1e-6)\n",
    "            log_probs = log_probs.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "        return actions, log_probs\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.network = MLP(obs_dim + action_dim, 1, hidden_dims)\n",
    "    \n",
    "    def forward(self, obs, action):\n",
    "        x = torch.cat([obs, action], dim=-1)\n",
    "        return self.network(x)\n",
    "\n",
    "class TwinQNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.q1 = QNetwork(obs_dim, action_dim, hidden_dims)\n",
    "        self.q2 = QNetwork(obs_dim, action_dim, hidden_dims)\n",
    "    \n",
    "    def forward(self, obs, action):\n",
    "        return self.q1(obs, action), self.q2(obs, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fb2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim: int, action_dim: int, max_size: int, device: str):\n",
    "        self.max_size = max_size\n",
    "        self.device = device\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        self.obs = torch.zeros((max_size, obs_dim), dtype=torch.float32)\n",
    "        self.actions = torch.zeros((max_size, action_dim), dtype=torch.float32)\n",
    "        self.rewards = torch.zeros((max_size, 1), dtype=torch.float32)\n",
    "        self.next_obs = torch.zeros((max_size, obs_dim), dtype=torch.float32)\n",
    "        self.dones = torch.zeros((max_size, 1), dtype=torch.float32)\n",
    "        \n",
    "        self._prev_timestep = None\n",
    "        \n",
    "    def add(self, timestep, action=None):\n",
    "        if action is not None and self._prev_timestep is not None:\n",
    "            obs = torch.from_numpy(self._prev_timestep.observation.astype(np.float32))\n",
    "            next_obs = torch.from_numpy(timestep.observation.astype(np.float32))\n",
    "            action_tensor = torch.from_numpy(action.astype(np.float32))\n",
    "            reward = torch.tensor(float(timestep.reward), dtype=torch.float32).unsqueeze(0)\n",
    "            done = torch.tensor(float(1.0 - timestep.discount), dtype=torch.float32).unsqueeze(0)\n",
    "            \n",
    "            self.obs[self.ptr] = obs\n",
    "            self.actions[self.ptr] = action_tensor\n",
    "            self.rewards[self.ptr] = reward\n",
    "            self.next_obs[self.ptr] = next_obs\n",
    "            self.dones[self.ptr] = done\n",
    "            \n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "            self.size = min(self.size + 1, self.max_size)\n",
    "            \n",
    "        self._prev_timestep = timestep\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        indices = torch.randint(0, self.size, (batch_size,))\n",
    "        return (\n",
    "            self.obs[indices].to(self.device),\n",
    "            self.actions[indices].to(self.device),\n",
    "            self.rewards[indices].to(self.device),\n",
    "            self.next_obs[indices].to(self.device),\n",
    "            self.dones[indices].to(self.device)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self, obs_dim: int, action_dim: int, args: TrainingArgs):\n",
    "        self.device = torch.device(args.device)\n",
    "        self.discount = args.discount\n",
    "        self.tau = args.tau\n",
    "        self.target_entropy = -action_dim\n",
    "        \n",
    "        # Networks\n",
    "        self.actor = Policy(obs_dim, action_dim, args.hidden_dims).to(self.device)\n",
    "        self.critic = TwinQNetwork(obs_dim, action_dim, args.hidden_dims).to(self.device)\n",
    "        self.target_critic = TwinQNetwork(obs_dim, action_dim, args.hidden_dims).to(self.device)\n",
    "        self.actor = self.actor.float()\n",
    "        self.critic = self.critic.float()\n",
    "        self.target_critic = self.target_critic.float()\n",
    "        \n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        self.log_alpha = torch.tensor(np.log(args.init_temperature), dtype=torch.float32, requires_grad=True, device=self.device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=args.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=args.critic_lr)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=args.temp_lr)\n",
    "        \n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self.log_alpha.exp()\n",
    "    \n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.from_numpy(obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "            action, _ = self.actor.sample(obs_tensor, deterministic=deterministic)\n",
    "            return action.cpu().numpy()[0].astype(np.float32)\n",
    "    \n",
    "    def update(self, replay_buffer: ReplayBuffer, batch_size: int):\n",
    "        obs, actions, rewards, next_obs, dones = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        obs = obs.float()\n",
    "        actions = actions.float()\n",
    "        rewards = rewards.float()\n",
    "        next_obs = next_obs.float()\n",
    "        dones = dones.float()\n",
    "        \n",
    "        # Update critic\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actor.sample(next_obs)\n",
    "            target_q1, target_q2 = self.target_critic(next_obs, next_actions)\n",
    "            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs\n",
    "            target_q = rewards + (1 - dones) * torch.tensor(self.discount, dtype=torch.float32, device=self.device) * target_q\n",
    "        \n",
    "        current_q1, current_q2 = self.critic(obs, actions)\n",
    "        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update actor\n",
    "        new_actions, log_probs = self.actor.sample(obs)\n",
    "        q1, q2 = self.critic(obs, new_actions)\n",
    "        q = torch.min(q1, q2)\n",
    "        actor_loss = (self.alpha * log_probs - q).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update temperature\n",
    "        target_entropy_tensor = torch.tensor(self.target_entropy, dtype=torch.float32, device=self.device)\n",
    "        alpha_loss = -(self.log_alpha * (log_probs + target_entropy_tensor).detach()).mean()\n",
    "        \n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.soft_update(self.critic, self.target_critic)\n",
    "        \n",
    "        return {\n",
    "            \"critic_loss\": critic_loss.item(),\n",
    "            \"actor_loss\": actor_loss.item(),\n",
    "            \"alpha_loss\": alpha_loss.item(),\n",
    "            \"alpha\": self.alpha.item(),\n",
    "        }\n",
    "    \n",
    "    def soft_update(self, source, target):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        torch.save({\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "            'target_critic': self.target_critic.state_dict(),\n",
    "            'log_alpha': self.log_alpha,\n",
    "        }, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "        self.target_critic.load_state_dict(checkpoint['target_critic'])\n",
    "        self.log_alpha = checkpoint['log_alpha']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82008f",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(args: TrainingArgs, record_dir: Optional[Path] = None):\n",
    "    env = suite.load(\n",
    "        environment_name=args.environment_name,\n",
    "        seed=args.seed,\n",
    "        task_kwargs=dict(\n",
    "            n_steps_lookahead=args.n_steps_lookahead,\n",
    "            trim_silence=args.trim_silence,\n",
    "            gravity_compensation=args.gravity_compensation,\n",
    "            reduced_action_space=args.reduced_action_space,\n",
    "            control_timestep=args.control_timestep,\n",
    "            primitive_fingertip_collisions=args.primitive_fingertip_collisions,\n",
    "            change_color_on_activation=True,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    if record_dir is not None and not args.hpc_mode:\n",
    "        env = robopianist_wrappers.PianoSoundVideoWrapper(\n",
    "            environment=env,\n",
    "            record_dir=record_dir,\n",
    "            record_every=1,\n",
    "            camera_id=\"piano/back\",\n",
    "            height=480,\n",
    "            width=640,\n",
    "        )\n",
    "    \n",
    "    env = wrappers.EpisodeStatisticsWrapper(environment=env, deque_size=1)\n",
    "    env = robopianist_wrappers.MidiEvaluationWrapper(environment=env, deque_size=1)\n",
    "    \n",
    "    if args.action_reward_observation:\n",
    "        env = wrappers.ObservationActionRewardWrapper(env)\n",
    "    \n",
    "    env = wrappers.ConcatObservationWrapper(env)\n",
    "    env = wrappers.CanonicalSpecWrapper(env, clip=True)\n",
    "    env = wrappers.SinglePrecisionWrapper(env)\n",
    "    env = wrappers.DmControlWrapper(env)\n",
    "    \n",
    "    return env\n",
    "\n",
    "# Test environment creation\n",
    "print(\"Creating test environment...\")\n",
    "test_env = get_env(args)\n",
    "test_timestep = test_env.reset()\n",
    "\n",
    "obs_dim = test_timestep.observation.shape[0]\n",
    "action_dim = test_env.action_spec().shape[0]\n",
    "\n",
    "print(f\"Observation dimension: {obs_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198fff8e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args: TrainingArgs):\n",
    "    \"\"\"Train the SAC agent.\"\"\"\n",
    "    \n",
    "    # Set random seeds\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    print(f\"Using device: {args.device}\")\n",
    "    \n",
    "    # Create experiment directory\n",
    "    run_name = f\"PyTorch-SAC-{args.environment_name}-{args.seed}-{int(time.time())}\"\n",
    "    experiment_dir = Path(args.root_dir) / run_name\n",
    "    experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Experiment directory: {experiment_dir}\")\n",
    "    \n",
    "    # Initialize wandb\n",
    "    if args.mode != \"disabled\":\n",
    "        wandb.init(\n",
    "            project=args.project,\n",
    "            config=asdict(args),\n",
    "            mode=args.mode,\n",
    "            name=run_name,\n",
    "        )\n",
    "    \n",
    "    # Create environments\n",
    "    env = get_env(args)\n",
    "    eval_env = get_env(args, record_dir=experiment_dir / \"eval\")\n",
    "    \n",
    "    # Initialize SAC agent\n",
    "    agent = SAC(obs_dim, action_dim, args)\n",
    "    \n",
    "    # Initialize replay buffer\n",
    "    replay_buffer = ReplayBuffer(obs_dim, action_dim, args.replay_capacity, args.device)\n",
    "    \n",
    "    # Training metrics\n",
    "    training_metrics = {\n",
    "        'steps': [],\n",
    "        'critic_losses': [],\n",
    "        'actor_losses': [],\n",
    "        'alpha_losses': [],\n",
    "        'alphas': [],\n",
    "        'eval_returns': [],\n",
    "        'eval_f1_scores': [], \n",
    "        'eval_precision_scores': [], \n",
    "        'eval_recall_scores': [], \n",
    "        'eval_steps': []\n",
    "    }\n",
    "    \n",
    "    # Add initial timestep\n",
    "    timestep = env.reset()\n",
    "    replay_buffer.add(timestep)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    eval_count = 0 \n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"Starting training loop for {args.max_steps:,} steps...\")\n",
    "    progress_bar = tqdm(range(1, args.max_steps + 1), desc=\"Training\", unit=\"steps\")\n",
    "    \n",
    "    for step in progress_bar:\n",
    "        # Select action\n",
    "        if step < args.warmstart_steps:\n",
    "            action = env.action_spec().generate_value()\n",
    "        else:\n",
    "            action = agent.select_action(timestep.observation)\n",
    "        \n",
    "        # Step environment\n",
    "        timestep = env.step(action)\n",
    "        replay_buffer.add(timestep, action)\n",
    "        \n",
    "        # Reset if episode ended\n",
    "        if timestep.last():\n",
    "            if args.mode != \"disabled\":\n",
    "                wandb.log(prefix_dict(\"train\", env.get_statistics()), step=step)\n",
    "            timestep = env.reset()\n",
    "            replay_buffer.add(timestep)\n",
    "        \n",
    "        # Update agent\n",
    "        if step >= args.warmstart_steps and len(replay_buffer) >= args.batch_size:\n",
    "            metrics = agent.update(replay_buffer, args.batch_size)\n",
    "            \n",
    "            # Store and update progress bar\n",
    "            if step % args.log_interval == 0:\n",
    "                training_metrics['steps'].append(step)\n",
    "                training_metrics['critic_losses'].append(metrics['critic_loss'])\n",
    "                training_metrics['actor_losses'].append(metrics['actor_loss'])\n",
    "                training_metrics['alpha_losses'].append(metrics['alpha_loss'])\n",
    "                training_metrics['alphas'].append(metrics['alpha'])\n",
    "                \n",
    "                # Update progress bar description with latest metrics\n",
    "                progress_bar.set_postfix({\n",
    "                    'Critic': f\"{metrics['critic_loss']:.3f}\",\n",
    "                    'Actor': f\"{metrics['actor_loss']:.3f}\",\n",
    "                    'Alpha': f\"{metrics['alpha']:.3f}\",\n",
    "                    'Buffer': f\"{len(replay_buffer)}\"\n",
    "                })\n",
    "                \n",
    "                if args.mode != \"disabled\":\n",
    "                    wandb.log(prefix_dict(\"train\", metrics), step=step)\n",
    "        \n",
    "        if step % args.eval_interval == 0:\n",
    "            eval_count += 1 \n",
    "            \n",
    "            eval_returns = []\n",
    "            eval_f1s = []\n",
    "            eval_precisions = []\n",
    "            eval_recalls = []\n",
    "            \n",
    "            actual_eval_episodes = args.eval_episodes\n",
    "            \n",
    "            for episode in range(actual_eval_episodes):\n",
    "                episode_eval_env = get_env(args, record_dir=experiment_dir / \"eval\")\n",
    "                eval_timestep = episode_eval_env.reset()\n",
    "                eval_return = 0.0\n",
    "                \n",
    "                use_deterministic = (episode == 0) if actual_eval_episodes > 1 else True\n",
    "                \n",
    "                while not eval_timestep.last():\n",
    "                    eval_action = agent.select_action(eval_timestep.observation, deterministic=use_deterministic)\n",
    "                    eval_timestep = episode_eval_env.step(eval_action)\n",
    "                    eval_return += eval_timestep.reward\n",
    "                \n",
    "                eval_returns.append(eval_return)\n",
    "                \n",
    "                try:\n",
    "                    musical_metrics = episode_eval_env.get_musical_metrics()\n",
    "                    eval_f1s.append(musical_metrics['f1'])\n",
    "                    eval_precisions.append(musical_metrics['precision'])\n",
    "                    eval_recalls.append(musical_metrics['recall'])\n",
    "                except (AttributeError, ValueError):\n",
    "                    eval_f1s.append(0.0)\n",
    "                    eval_precisions.append(0.0)\n",
    "                    eval_recalls.append(0.0)\n",
    "            \n",
    "            mean_return = np.mean(eval_returns)\n",
    "            mean_f1 = np.mean(eval_f1s)\n",
    "            mean_precision = np.mean(eval_precisions)\n",
    "            mean_recall = np.mean(eval_recalls)\n",
    "            \n",
    "            # Calculate standard deviations\n",
    "            std_return = np.std(eval_returns) if len(eval_returns) > 1 else 0.0\n",
    "            std_f1 = np.std(eval_f1s) if len(eval_f1s) > 1 else 0.0\n",
    "            std_precision = np.std(eval_precisions) if len(eval_precisions) > 1 else 0.0\n",
    "            std_recall = np.std(eval_recalls) if len(eval_recalls) > 1 else 0.0\n",
    "            \n",
    "            # Store metrics\n",
    "            training_metrics['eval_returns'].append(mean_return)\n",
    "            training_metrics['eval_f1_scores'].append(mean_f1)\n",
    "            training_metrics['eval_precision_scores'].append(mean_precision)\n",
    "            training_metrics['eval_recall_scores'].append(mean_recall)\n",
    "            training_metrics['eval_steps'].append(step)\n",
    "            \n",
    "            if args.mode != \"disabled\":\n",
    "                eval_metrics = {\n",
    "                    \"eval/return_mean\": mean_return,\n",
    "                    \"eval/return_std\": std_return,\n",
    "                    \"eval/f1_mean\": mean_f1,\n",
    "                    \"eval/f1_std\": std_f1,\n",
    "                    \"eval/precision_mean\": mean_precision,\n",
    "                    \"eval/precision_std\": std_precision,\n",
    "                    \"eval/recall_mean\": mean_recall,\n",
    "                    \"eval/recall_std\": std_recall,\n",
    "                }\n",
    "                eval_metrics.update(prefix_dict(\"eval\", eval_env.get_statistics()))\n",
    "                wandb.log(eval_metrics, step=step)\n",
    "            \n",
    "            eval_info = f\"Eval #{eval_count}: R={mean_return:.1f}, F1={mean_f1:.4f}\"\n",
    "            if std_return > 0:\n",
    "                eval_info += f\"¬±{std_return:.1f}\"\n",
    "            if std_f1 > 0:\n",
    "                eval_info += f\"¬±{std_f1:.4f}\"\n",
    "                \n",
    "            progress_bar.set_postfix({\n",
    "                'Last_Eval': eval_info,\n",
    "                'Buffer': f\"{len(replay_buffer)}\"\n",
    "            })\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Total evaluations performed: {eval_count}\")\n",
    "    \n",
    "    # Save final model\n",
    "    model_path = experiment_dir / \"final_model.pt\"\n",
    "    agent.save(model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    \n",
    "    if args.mode != \"disabled\":\n",
    "        wandb.finish()\n",
    "    \n",
    "    return agent, training_metrics, experiment_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537cf2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Golden Hour for {args.max_steps:,} steps with {args.warmstart_steps:,} warmstart steps\")\n",
    "print(f\"Batch size: {args.batch_size}, Replay capacity: {args.replay_capacity:,}\")\n",
    "print(f\"Evaluation interval: {args.eval_interval:,} steps\")\n",
    "print(f\"Evaluation episodes per interval: {args.eval_episodes}\")\n",
    "\n",
    "# Run Golden Hour training instead of Twinkle Twinkle\n",
    "golden_hour_agent, golden_hour_metrics, golden_hour_exp_dir = train_golden_hour(args)\n",
    "\n",
    "print(\"Golden Hour training completed!\")\n",
    "print(f\"Experiment directory: {golden_hour_exp_dir}\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Final Golden Hour F1: {golden_hour_metrics['eval_f1_scores'][-1]:.4f}\")\n",
    "print(f\"  Final Golden Hour Return: {golden_hour_metrics['eval_returns'][-1]:.2f}\")\n",
    "print(f\"  Best Golden Hour F1: {max(golden_hour_metrics['eval_f1_scores']):.4f}\")\n",
    "print(f\"  Best Golden Hour Return: {max(golden_hour_metrics['eval_returns']):.2f}\")\n",
    "print(f\"  F1 evaluations: {len(golden_hour_metrics['eval_f1_scores'])} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944d9b8e",
   "metadata": {},
   "source": [
    "## Model Evaluation and Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced visualization with Golden Hour F1 scores\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Critic Loss\n",
    "if 'steps' in golden_hour_metrics and 'critic_losses' in golden_hour_metrics:\n",
    "    axes[0, 0].plot(golden_hour_metrics['steps'], golden_hour_metrics['critic_losses'])\n",
    "    axes[0, 0].set_title('Golden Hour Critic Loss')\n",
    "    axes[0, 0].set_xlabel('Steps')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True)\n",
    "else:\n",
    "    axes[0, 0].text(0.5, 0.5, 'No critic loss data', ha='center', va='center')\n",
    "    axes[0, 0].set_title('Golden Hour Critic Loss')\n",
    "\n",
    "# Actor Loss\n",
    "if 'steps' in golden_hour_metrics and 'actor_losses' in golden_hour_metrics:\n",
    "    axes[0, 1].plot(golden_hour_metrics['steps'], golden_hour_metrics['actor_losses'])\n",
    "    axes[0, 1].set_title('Golden Hour Actor Loss')\n",
    "    axes[0, 1].set_xlabel('Steps')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].grid(True)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'No actor loss data', ha='center', va='center')\n",
    "    axes[0, 1].set_title('Golden Hour Actor Loss')\n",
    "\n",
    "# Alpha (Temperature)\n",
    "if 'steps' in golden_hour_metrics and 'alphas' in golden_hour_metrics:\n",
    "    axes[0, 2].plot(golden_hour_metrics['steps'], golden_hour_metrics['alphas'])\n",
    "    axes[0, 2].set_title('Golden Hour Temperature (Alpha)')\n",
    "    axes[0, 2].set_xlabel('Steps')\n",
    "    axes[0, 2].set_ylabel('Alpha')\n",
    "    axes[0, 2].grid(True)\n",
    "else:\n",
    "    axes[0, 2].text(0.5, 0.5, 'No alpha data', ha='center', va='center')\n",
    "    axes[0, 2].set_title('Golden Hour Temperature (Alpha)')\n",
    "\n",
    "# Evaluation Returns\n",
    "if 'eval_returns' in golden_hour_metrics and golden_hour_metrics['eval_returns']:\n",
    "    axes[1, 0].plot(golden_hour_metrics['eval_steps'], golden_hour_metrics['eval_returns'], 'o-', color='blue', label='Returns')\n",
    "    axes[1, 0].set_title('Golden Hour Evaluation Returns')\n",
    "    axes[1, 0].set_xlabel('Steps')\n",
    "    axes[1, 0].set_ylabel('Return')\n",
    "    axes[1, 0].grid(True)\n",
    "    axes[1, 0].legend()\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No evaluation data', ha='center', va='center')\n",
    "    axes[1, 0].set_title('Golden Hour Evaluation Returns')\n",
    "\n",
    "# F1 Scores During Training\n",
    "if 'eval_f1_scores' in golden_hour_metrics and golden_hour_metrics['eval_f1_scores']:\n",
    "    axes[1, 1].plot(golden_hour_metrics['eval_steps'], golden_hour_metrics['eval_f1_scores'], 'o-', color='gold', label='Golden Hour F1')\n",
    "    axes[1, 1].set_title('Golden Hour F1 Scores')\n",
    "    axes[1, 1].set_xlabel('Steps')\n",
    "    axes[1, 1].set_ylabel('F1 Score')\n",
    "    axes[1, 1].grid(True)\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # Print F1 progression\n",
    "    print(\"Golden Hour F1 Score Progression During Training:\")\n",
    "    for step, f1 in zip(golden_hour_metrics['eval_steps'], golden_hour_metrics['eval_f1_scores']):\n",
    "        print(f\"  Step {step}: F1 = {f1:.4f}\")\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No F1 score data\\n(Run Golden Hour training first)', ha='center', va='center')\n",
    "    axes[1, 1].set_title('Golden Hour F1 Scores')\n",
    "\n",
    "# Musical Metrics Combined\n",
    "if ('eval_precision_scores' in golden_hour_metrics and golden_hour_metrics['eval_precision_scores'] and \n",
    "    'eval_recall_scores' in golden_hour_metrics and golden_hour_metrics['eval_recall_scores']):\n",
    "    \n",
    "    axes[1, 2].plot(golden_hour_metrics['eval_steps'], golden_hour_metrics['eval_precision_scores'], 'o-', color='purple', label='Precision')\n",
    "    axes[1, 2].plot(golden_hour_metrics['eval_steps'], golden_hour_metrics['eval_recall_scores'], 'o-', color='orange', label='Recall')\n",
    "    axes[1, 2].set_title('Golden Hour Precision & Recall')\n",
    "    axes[1, 2].set_xlabel('Steps')\n",
    "    axes[1, 2].set_ylabel('Score')\n",
    "    axes[1, 2].grid(True)\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    # Print musical metrics progression\n",
    "    print(\"\\nGolden Hour Musical Metrics Progression During Training:\")\n",
    "    for step, p, r in zip(golden_hour_metrics['eval_steps'], golden_hour_metrics['eval_precision_scores'], golden_hour_metrics['eval_recall_scores']):\n",
    "        print(f\"  Step {step}: Precision = {p:.4f}, Recall = {r:.4f}\")\n",
    "else:\n",
    "    axes[1, 2].text(0.5, 0.5, 'No musical metrics\\n(Run Golden Hour training first)', ha='center', va='center')\n",
    "    axes[1, 2].set_title('Golden Hour Precision & Recall')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of Golden Hour training results\n",
    "if 'eval_f1_scores' in golden_hour_metrics and golden_hour_metrics['eval_f1_scores']:\n",
    "    print(f\"\\nGolden Hour Training Summary:\")\n",
    "    print(f\"Final F1 Score: {golden_hour_metrics['eval_f1_scores'][-1]:.4f}\")\n",
    "    print(f\"Best F1 Score: {max(golden_hour_metrics['eval_f1_scores']):.4f}\")\n",
    "    print(f\"F1 Score Improvement: {golden_hour_metrics['eval_f1_scores'][-1] - golden_hour_metrics['eval_f1_scores'][0]:+.4f}\")\n",
    "    \n",
    "    if 'eval_returns' in golden_hour_metrics and golden_hour_metrics['eval_returns']:\n",
    "        print(f\"Final Return: {golden_hour_metrics['eval_returns'][-1]:.2f}\")\n",
    "        print(f\"Best Return: {max(golden_hour_metrics['eval_returns']):.2f}\")\n",
    "        print(f\"Return Improvement: {golden_hour_metrics['eval_returns'][-1] - golden_hour_metrics['eval_returns'][0]:+.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pianist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125d74af",
   "metadata": {},
   "source": [
    "# RoboPianist Training and Evaluation on ROBOPIANIST-REPERTOIRE-150\n",
    "\n",
    "This notebook tests RoboPianist’s ability to reproduce the papers results. We have ported the original kevinzakka/robopianist-rl (https://github.com/kevinzakka/robopianist-rl.git) training pipeline from JAX to PyTorch and adapted the structure of the official tutorial.ipynb (https://github.com/google-research/robopianist/blob/main/tutorial.ipynb) for easy to use and full reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98d4a6",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "1. Follow the [RoboPianist installation instructions](https://github.com/google-research/robopianist?tab=readme-ov-file#installation) to set up the pianist conda environmen.\n",
    "2. Intall the following packages in the pianist conda environment:\n",
    "   ```bash\n",
    "   conda activate pianist\n",
    "   conda install \"numpy<2.0.0\"\n",
    "   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "   conda install -c conda-forge matplotlib tqdm wandb tyro\n",
    "   pip install wandb\n",
    "   ```\n",
    "3. To verify that everything is installed and downloaded, please run once the [tutorial.ipynb](robopianist/tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62a6e3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b1443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:38:39.711074Z",
     "iopub.status.busy": "2025-07-29T18:38:39.710691Z",
     "iopub.status.idle": "2025-07-29T18:39:05.197759Z",
     "shell.execute_reply": "2025-07-29T18:39:05.196767Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import time\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from tqdm import tqdm\n",
    "import tyro\n",
    "from IPython.display import HTML, clear_output\n",
    "from base64 import b64encode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_dir = os.getcwd()  \n",
    "robopianist_dir = os.path.join(base_dir, \"robopianist\")\n",
    "os.chdir(robopianist_dir)\n",
    "if robopianist_dir not in sys.path:\n",
    "    sys.path.insert(0, robopianist_dir)\n",
    "\n",
    "# RoboPianist imports\n",
    "from robopianist import suite, music\n",
    "import dm_env_wrappers as wrappers\n",
    "import robopianist.wrappers as robopianist_wrappers\n",
    "import dm_env\n",
    "\n",
    "os.chdir(base_dir)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7e154",
   "metadata": {},
   "source": [
    "## Configuration and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e91a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:05.202184Z",
     "iopub.status.busy": "2025-07-29T18:39:05.201783Z",
     "iopub.status.idle": "2025-07-29T18:39:05.211690Z",
     "shell.execute_reply": "2025-07-29T18:39:05.211144Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingArgs:\n",
    "    hpc_mode: bool = False\n",
    "    \n",
    "    # Environment settings\n",
    "    environment_name: str = \"RoboPianist-debug-TwinkleTwinkleRousseau-v0\"\n",
    "    seed: int = 42\n",
    "    control_timestep: float = 0.05\n",
    "    n_steps_lookahead: int = 10\n",
    "    trim_silence: bool = True\n",
    "    gravity_compensation: bool = True\n",
    "    reduced_action_space: bool = True\n",
    "    primitive_fingertip_collisions: bool = True\n",
    "    action_reward_observation: bool = True\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    max_steps: int = 5000000\n",
    "    warmstart_steps: int = 5000\n",
    "    batch_size: int = 256\n",
    "    replay_capacity: int = 5000000\n",
    "    \n",
    "    # SAC hyperparameters\n",
    "    actor_lr: float = 3e-4\n",
    "    critic_lr: float = 3e-4\n",
    "    temp_lr: float = 3e-4\n",
    "    hidden_dims: Tuple[int, ...] = (256, 256, 256)\n",
    "    discount: float = 0.88\n",
    "    tau: float = 0.005\n",
    "    init_temperature: float = 1.0\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    log_interval: int = 1000\n",
    "    eval_interval: int = 10000\n",
    "    eval_episodes: int = 1\n",
    "    tqdm_bar: bool = True\n",
    "    \n",
    "    # Paths and wandb\n",
    "    base_dir = os.getcwd()\n",
    "    parent_dir = os.path.dirname(base_dir)\n",
    "    root_dir: str = str(os.path.join(parent_dir, \"runs\"))\n",
    "    project: str = \"robopianist-pytorch\"\n",
    "    mode: str = \"disabled\"\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create training configuration\n",
    "args = TrainingArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa369495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:05.214249Z",
     "iopub.status.busy": "2025-07-29T18:39:05.214051Z",
     "iopub.status.idle": "2025-07-29T18:39:05.223252Z",
     "shell.execute_reply": "2025-07-29T18:39:05.222707Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_video(filename: str):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Video file not found: {filename}\")\n",
    "        return None\n",
    "    \n",
    "    mp4 = open(filename, \"rb\").read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    \n",
    "    return HTML(f\"\"\"\n",
    "    <video controls width=\"640\" height=\"480\">\n",
    "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\")\n",
    "\n",
    "def prefix_dict(prefix: str, d: dict) -> dict:\n",
    "    return {f\"{prefix}/{k}\": v for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a593865",
   "metadata": {},
   "source": [
    "## Policy Network in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403aa79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:05.226601Z",
     "iopub.status.busy": "2025-07-29T18:39:05.226404Z",
     "iopub.status.idle": "2025-07-29T18:39:05.239123Z",
     "shell.execute_reply": "2025-07-29T18:39:05.238602Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            current_dim = hidden_dim\n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.backbone = MLP(obs_dim, 2 * action_dim, hidden_dims)\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        outputs = self.backbone(obs)\n",
    "        means, log_stds = torch.chunk(outputs, 2, dim=-1)\n",
    "        log_stds = torch.clamp(log_stds, -20, 2)\n",
    "        return means, log_stds\n",
    "    \n",
    "    def sample(self, obs, deterministic=False):\n",
    "        means, log_stds = self.forward(obs)\n",
    "        stds = torch.exp(log_stds)\n",
    "        \n",
    "        if deterministic:\n",
    "            actions = torch.tanh(means)\n",
    "            log_probs = None\n",
    "        else:\n",
    "            dist = Normal(means, stds)\n",
    "            samples = dist.rsample()\n",
    "            actions = torch.tanh(samples)\n",
    "            \n",
    "            log_probs = dist.log_prob(samples)\n",
    "            log_probs -= torch.log(1 - actions.pow(2) + 1e-6)\n",
    "            log_probs = log_probs.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "        return actions, log_probs\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.network = MLP(obs_dim + action_dim, 1, hidden_dims)\n",
    "    \n",
    "    def forward(self, obs, action):\n",
    "        x = torch.cat([obs, action], dim=-1)\n",
    "        return self.network(x)\n",
    "\n",
    "class TwinQNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.q1 = QNetwork(obs_dim, action_dim, hidden_dims)\n",
    "        self.q2 = QNetwork(obs_dim, action_dim, hidden_dims)\n",
    "    \n",
    "    def forward(self, obs, action):\n",
    "        return self.q1(obs, action), self.q2(obs, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fb2d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:05.240777Z",
     "iopub.status.busy": "2025-07-29T18:39:05.240578Z",
     "iopub.status.idle": "2025-07-29T18:39:05.252528Z",
     "shell.execute_reply": "2025-07-29T18:39:05.251791Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim: int, action_dim: int, max_size: int, device: str):\n",
    "        self.max_size = max_size\n",
    "        self.device = device\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        self.obs = torch.zeros((max_size, obs_dim), dtype=torch.float32)\n",
    "        self.actions = torch.zeros((max_size, action_dim), dtype=torch.float32)\n",
    "        self.rewards = torch.zeros((max_size, 1), dtype=torch.float32)\n",
    "        self.next_obs = torch.zeros((max_size, obs_dim), dtype=torch.float32)\n",
    "        self.dones = torch.zeros((max_size, 1), dtype=torch.float32)\n",
    "        \n",
    "        self._prev_timestep = None\n",
    "        \n",
    "    def add(self, timestep, action=None):\n",
    "        if action is not None and self._prev_timestep is not None:\n",
    "            obs = torch.from_numpy(self._prev_timestep.observation.astype(np.float32))\n",
    "            next_obs = torch.from_numpy(timestep.observation.astype(np.float32))\n",
    "            action_tensor = torch.from_numpy(action.astype(np.float32))\n",
    "            reward = torch.tensor(float(timestep.reward), dtype=torch.float32).unsqueeze(0)\n",
    "            done = torch.tensor(float(1.0 - timestep.discount), dtype=torch.float32).unsqueeze(0)\n",
    "            \n",
    "            self.obs[self.ptr] = obs\n",
    "            self.actions[self.ptr] = action_tensor\n",
    "            self.rewards[self.ptr] = reward\n",
    "            self.next_obs[self.ptr] = next_obs\n",
    "            self.dones[self.ptr] = done\n",
    "            \n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "            self.size = min(self.size + 1, self.max_size)\n",
    "            \n",
    "        self._prev_timestep = timestep\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        indices = torch.randint(0, self.size, (batch_size,))\n",
    "        return (\n",
    "            self.obs[indices].to(self.device),\n",
    "            self.actions[indices].to(self.device),\n",
    "            self.rewards[indices].to(self.device),\n",
    "            self.next_obs[indices].to(self.device),\n",
    "            self.dones[indices].to(self.device)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0a2ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:05.256141Z",
     "iopub.status.busy": "2025-07-29T18:39:05.255947Z",
     "iopub.status.idle": "2025-07-29T18:39:05.270410Z",
     "shell.execute_reply": "2025-07-29T18:39:05.269614Z"
    }
   },
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self, obs_dim: int, action_dim: int, args: TrainingArgs):\n",
    "        self.device = torch.device(args.device)\n",
    "        self.discount = args.discount\n",
    "        self.tau = args.tau\n",
    "        self.target_entropy = -action_dim\n",
    "        \n",
    "        # Networks\n",
    "        self.actor = Policy(obs_dim, action_dim, args.hidden_dims).to(self.device)\n",
    "        self.critic = TwinQNetwork(obs_dim, action_dim, args.hidden_dims).to(self.device)\n",
    "        self.target_critic = TwinQNetwork(obs_dim, action_dim, args.hidden_dims).to(self.device)\n",
    "        self.actor = self.actor.float()\n",
    "        self.critic = self.critic.float()\n",
    "        self.target_critic = self.target_critic.float()\n",
    "        \n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        self.log_alpha = torch.tensor(np.log(args.init_temperature), dtype=torch.float32, requires_grad=True, device=self.device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=args.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=args.critic_lr)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=args.temp_lr)\n",
    "        \n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self.log_alpha.exp()\n",
    "    \n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.from_numpy(obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "            action, _ = self.actor.sample(obs_tensor, deterministic=deterministic)\n",
    "            return action.cpu().numpy()[0].astype(np.float32)\n",
    "    \n",
    "    def update(self, replay_buffer: ReplayBuffer, batch_size: int):\n",
    "        obs, actions, rewards, next_obs, dones = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        obs = obs.float()\n",
    "        actions = actions.float()\n",
    "        rewards = rewards.float()\n",
    "        next_obs = next_obs.float()\n",
    "        dones = dones.float()\n",
    "        \n",
    "        # Update critic\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actor.sample(next_obs)\n",
    "            target_q1, target_q2 = self.target_critic(next_obs, next_actions)\n",
    "            # JAX implementation uses the mean of the two Q-values, not the minimum for the critic target.\n",
    "            target_q_unclipped = (target_q1 + target_q2) / 2.0\n",
    "            target_q = target_q_unclipped - self.alpha.detach() * next_log_probs\n",
    "            target_q = rewards + (1 - dones) * self.discount * target_q\n",
    "        \n",
    "        current_q1, current_q2 = self.critic(obs, actions)\n",
    "        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update actor\n",
    "        new_actions, log_probs = self.actor.sample(obs)\n",
    "        q1, q2 = self.critic(obs, new_actions)\n",
    "        # JAX implementation uses the mean of the two Q-values, not the minimum.\n",
    "        q = (q1 + q2) / 2.0\n",
    "        actor_loss = (self.alpha.detach() * log_probs - q).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update temperature\n",
    "        alpha_loss = (self.alpha * (-log_probs - self.target_entropy).detach()).mean()\n",
    "        \n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.soft_update(self.critic, self.target_critic)\n",
    "        \n",
    "        return {\n",
    "            \"critic_loss\": critic_loss.item(),\n",
    "            \"actor_loss\": actor_loss.item(),\n",
    "            \"alpha_loss\": alpha_loss.item(),\n",
    "            \"alpha\": self.alpha.item(),\n",
    "        }\n",
    "    \n",
    "    def soft_update(self, source, target):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        torch.save({\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "            'target_critic': self.target_critic.state_dict(),\n",
    "            'log_alpha': self.log_alpha,\n",
    "        }, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "        self.target_critic.load_state_dict(checkpoint['target_critic'])\n",
    "        self.log_alpha = checkpoint['log_alpha']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82008f",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790934a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:05.272600Z",
     "iopub.status.busy": "2025-07-29T18:39:05.272402Z",
     "iopub.status.idle": "2025-07-29T18:39:06.914945Z",
     "shell.execute_reply": "2025-07-29T18:39:06.914006Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_env(args: TrainingArgs, record_dir: Optional[Path] = None):\n",
    "    env = suite.load(\n",
    "        environment_name=args.environment_name,\n",
    "        seed=args.seed,\n",
    "        task_kwargs=dict(\n",
    "            n_steps_lookahead=args.n_steps_lookahead,\n",
    "            trim_silence=args.trim_silence,\n",
    "            gravity_compensation=args.gravity_compensation,\n",
    "            reduced_action_space=args.reduced_action_space,\n",
    "            control_timestep=args.control_timestep,\n",
    "            primitive_fingertip_collisions=args.primitive_fingertip_collisions,\n",
    "            change_color_on_activation=True,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    if record_dir is not None and not args.hpc_mode:\n",
    "        env = robopianist_wrappers.PianoSoundVideoWrapper(\n",
    "            environment=env,\n",
    "            record_dir=record_dir,\n",
    "            record_every=1,\n",
    "            camera_id=\"piano/back\",\n",
    "            height=480,\n",
    "            width=640,\n",
    "        )\n",
    "    \n",
    "    env = wrappers.EpisodeStatisticsWrapper(environment=env, deque_size=1)\n",
    "    env = robopianist_wrappers.MidiEvaluationWrapper(environment=env, deque_size=1)\n",
    "    \n",
    "    if args.action_reward_observation:\n",
    "        env = wrappers.ObservationActionRewardWrapper(env)\n",
    "    \n",
    "    env = wrappers.ConcatObservationWrapper(env)\n",
    "    env = wrappers.CanonicalSpecWrapper(env, clip=True)\n",
    "    env = wrappers.SinglePrecisionWrapper(env)\n",
    "    env = wrappers.DmControlWrapper(env)\n",
    "    \n",
    "    return env\n",
    "\n",
    "# Test environment creation\n",
    "print(\"Creating test environment...\")\n",
    "test_env = get_env(args)\n",
    "test_timestep = test_env.reset()\n",
    "\n",
    "obs_dim = test_timestep.observation.shape[0]\n",
    "action_dim = test_env.action_spec().shape[0]\n",
    "\n",
    "print(f\"Observation dimension: {obs_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198fff8e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa9384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:06.917299Z",
     "iopub.status.busy": "2025-07-29T18:39:06.917075Z",
     "iopub.status.idle": "2025-07-29T18:39:06.937991Z",
     "shell.execute_reply": "2025-07-29T18:39:06.937178Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(args: TrainingArgs):\n",
    "    # Set random seeds\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    print(f\"Using device: {args.device}\")\n",
    "    print(f\"HPC Mode: {'ENABLED' if args.hpc_mode else 'DISABLED'}\")\n",
    "    \n",
    "    # Create experiment directory\n",
    "    run_name = f\"PyTorch-Robopianist-{args.environment_name}-{args.seed}-{int(time.time())}\"\n",
    "    experiment_dir = Path(args.root_dir) / run_name\n",
    "    experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Experiment directory: {experiment_dir}\")\n",
    "    \n",
    "    # Initialize wandb\n",
    "    if args.mode != \"disabled\":\n",
    "        wandb.init(\n",
    "            project=args.project,\n",
    "            config=asdict(args),\n",
    "            mode=args.mode,\n",
    "            name=run_name,\n",
    "        )\n",
    "    \n",
    "    # Create environments\n",
    "    env = get_env(args)\n",
    "    eval_env = get_env(args, record_dir=experiment_dir / \"eval\")\n",
    "    \n",
    "    # Initialize SAC agent\n",
    "    agent = SAC(obs_dim, action_dim, args)\n",
    "    \n",
    "    # Initialize replay buffer\n",
    "    replay_buffer = ReplayBuffer(obs_dim, action_dim, args.replay_capacity, args.device)\n",
    "    \n",
    "    # Training metrics\n",
    "    training_metrics = {\n",
    "        'steps': [],\n",
    "        'critic_losses': [],\n",
    "        'actor_losses': [],\n",
    "        'alpha_losses': [],\n",
    "        'alphas': [],\n",
    "        'eval_returns': [],\n",
    "        'eval_f1_scores': [], \n",
    "        'eval_precision_scores': [], \n",
    "        'eval_recall_scores': [], \n",
    "        'eval_steps': []\n",
    "    }\n",
    "    \n",
    "    # Add initial timestep\n",
    "    timestep = env.reset()\n",
    "    replay_buffer.add(timestep)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    eval_count = 0 \n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"Starting training loop for {args.max_steps:,} steps...\")\n",
    "    progress_bar = tqdm(range(1, args.max_steps + 1), desc=\"Training\", unit=\"steps\")\n",
    "    \n",
    "    for step in progress_bar:\n",
    "        # Select action\n",
    "        if step < args.warmstart_steps:\n",
    "            action = env.action_spec().generate_value()\n",
    "        else:\n",
    "            action = agent.select_action(timestep.observation)\n",
    "        \n",
    "        # Step environment\n",
    "        timestep = env.step(action)\n",
    "        replay_buffer.add(timestep, action)\n",
    "        \n",
    "        # Reset if episode ended\n",
    "        if timestep.last():\n",
    "            if args.mode != \"disabled\":\n",
    "                wandb.log(prefix_dict(\"train\", env.get_statistics()), step=step)\n",
    "            timestep = env.reset()\n",
    "            replay_buffer.add(timestep)\n",
    "        \n",
    "        # Update agent\n",
    "        if step >= args.warmstart_steps and len(replay_buffer) >= args.batch_size:\n",
    "            metrics = agent.update(replay_buffer, args.batch_size)\n",
    "            \n",
    "            # Store and update progress bar\n",
    "            if step % args.log_interval == 0:\n",
    "                training_metrics['steps'].append(step)\n",
    "                training_metrics['critic_losses'].append(metrics['critic_loss'])\n",
    "                training_metrics['actor_losses'].append(metrics['actor_loss'])\n",
    "                training_metrics['alpha_losses'].append(metrics['alpha_loss'])\n",
    "                training_metrics['alphas'].append(metrics['alpha'])\n",
    "                \n",
    "                # Update progress bar description with latest metrics\n",
    "                progress_bar.set_postfix({\n",
    "                    'Critic': f\"{metrics['critic_loss']:.3f}\",\n",
    "                    'Actor': f\"{metrics['actor_loss']:.3f}\",\n",
    "                    'Alpha': f\"{metrics['alpha']:.3f}\",\n",
    "                    'Buffer': f\"{len(replay_buffer)}\"\n",
    "                })\n",
    "                \n",
    "                if args.mode != \"disabled\":\n",
    "                    wandb.log(prefix_dict(\"train\", metrics), step=step)\n",
    "        \n",
    "        if step % args.eval_interval == 0:\n",
    "            eval_count += 1 \n",
    "            \n",
    "            eval_returns = []\n",
    "            eval_f1s = []\n",
    "            eval_precisions = []\n",
    "            eval_recalls = []\n",
    "            \n",
    "            actual_eval_episodes = args.eval_episodes\n",
    "            \n",
    "            for episode in range(actual_eval_episodes):\n",
    "                episode_eval_env = get_env(args, record_dir=experiment_dir / \"eval\")\n",
    "                eval_timestep = episode_eval_env.reset()\n",
    "                eval_return = 0.0\n",
    "                \n",
    "                use_deterministic = (episode == 0) if actual_eval_episodes > 1 else True\n",
    "                \n",
    "                while not eval_timestep.last():\n",
    "                    eval_action = agent.select_action(eval_timestep.observation, deterministic=use_deterministic)\n",
    "                    eval_timestep = episode_eval_env.step(eval_action)\n",
    "                    eval_return += eval_timestep.reward\n",
    "                \n",
    "                eval_returns.append(eval_return)\n",
    "                \n",
    "                try:\n",
    "                    musical_metrics = episode_eval_env.get_musical_metrics()\n",
    "                    eval_f1s.append(musical_metrics['f1'])\n",
    "                    eval_precisions.append(musical_metrics['precision'])\n",
    "                    eval_recalls.append(musical_metrics['recall'])\n",
    "                except (AttributeError, ValueError):\n",
    "                    eval_f1s.append(0.0)\n",
    "                    eval_precisions.append(0.0)\n",
    "                    eval_recalls.append(0.0)\n",
    "            \n",
    "            mean_return = np.mean(eval_returns)\n",
    "            mean_f1 = np.mean(eval_f1s)\n",
    "            mean_precision = np.mean(eval_precisions)\n",
    "            mean_recall = np.mean(eval_recalls)\n",
    "            \n",
    "            # Calculate standard deviations\n",
    "            std_return = np.std(eval_returns) if len(eval_returns) > 1 else 0.0\n",
    "            std_f1 = np.std(eval_f1s) if len(eval_f1s) > 1 else 0.0\n",
    "            std_precision = np.std(eval_precisions) if len(eval_precisions) > 1 else 0.0\n",
    "            std_recall = np.std(eval_recalls) if len(eval_recalls) > 1 else 0.0\n",
    "            \n",
    "            # Store metrics\n",
    "            training_metrics['eval_returns'].append(mean_return)\n",
    "            training_metrics['eval_f1_scores'].append(mean_f1)\n",
    "            training_metrics['eval_precision_scores'].append(mean_precision)\n",
    "            training_metrics['eval_recall_scores'].append(mean_recall)\n",
    "            training_metrics['eval_steps'].append(step)\n",
    "            \n",
    "            if args.mode != \"disabled\":\n",
    "                eval_metrics = {\n",
    "                    \"eval/return_mean\": mean_return,\n",
    "                    \"eval/return_std\": std_return,\n",
    "                    \"eval/f1_mean\": mean_f1,\n",
    "                    \"eval/f1_std\": std_f1,\n",
    "                    \"eval/precision_mean\": mean_precision,\n",
    "                    \"eval/precision_std\": std_precision,\n",
    "                    \"eval/recall_mean\": mean_recall,\n",
    "                    \"eval/recall_std\": std_recall,\n",
    "                }\n",
    "                eval_metrics.update(prefix_dict(\"eval\", eval_env.get_statistics()))\n",
    "                wandb.log(eval_metrics, step=step)\n",
    "            \n",
    "            eval_info = f\"Eval #{eval_count}: R={mean_return:.1f}, F1={mean_f1:.4f}\"\n",
    "            if std_return > 0:\n",
    "                eval_info += f\"±{std_return:.1f}\"\n",
    "            if std_f1 > 0:\n",
    "                eval_info += f\"±{std_f1:.4f}\"\n",
    "                \n",
    "            progress_bar.set_postfix({\n",
    "                'Last_Eval': eval_info,\n",
    "                'Buffer': f\"{len(replay_buffer)}\"\n",
    "            })\n",
    "            \n",
    "            # Save model at every evaluation step in HPC mode\n",
    "            if args.hpc_mode:\n",
    "                model_path = experiment_dir / f\"model_step_{step}_eval_{eval_count}.pt\"\n",
    "                agent.save(model_path)\n",
    "                print(f\"Model saved at step {step}: {model_path}\")\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Total evaluations performed: {eval_count}\")\n",
    "    \n",
    "    # Save final model\n",
    "    model_path = experiment_dir / \"final_model.pt\"\n",
    "    agent.save(model_path)\n",
    "    print(f\"Final model saved to: {model_path}\")\n",
    "    \n",
    "    if args.hpc_mode:\n",
    "        print(f\"HPC Mode: {eval_count} evaluation models saved in {experiment_dir}\")\n",
    "        print(\"To generate videos locally later, set hpc_mode=False and reload saved models\")\n",
    "    \n",
    "    if args.mode != \"disabled\":\n",
    "        wandb.finish()\n",
    "    \n",
    "    return agent, training_metrics, experiment_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537cf2b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:06.940074Z",
     "iopub.status.busy": "2025-07-29T18:39:06.939868Z",
     "iopub.status.idle": "2025-07-29T18:49:01.837184Z",
     "shell.execute_reply": "2025-07-29T18:49:01.836603Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Training for {args.max_steps:,} steps with {args.warmstart_steps:,} warmstart steps\")\n",
    "print(f\"Batch size: {args.batch_size}, Replay capacity: {args.replay_capacity:,}\")\n",
    "print(f\"Evaluation interval: {args.eval_interval:,} steps\")\n",
    "print(f\"Evaluation episodes per interval: {args.eval_episodes}\")\n",
    "\n",
    "# Run enhanced training with clean output\n",
    "agent, metrics, exp_dir = train(args)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Experiment directory: {exp_dir}\")\n",
    "if args.hpc_mode:\n",
    "    print(\"HPC Mode was enabled - videos were not recorded to avoid rendering issues\")\n",
    "    print(\"Models were saved at every evaluation step for later video generation\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Final Training F1: {metrics['eval_f1_scores'][-1]:.4f}\")\n",
    "print(f\"  Final Training Return: {metrics['eval_returns'][-1]:.2f}\")\n",
    "print(f\"  Best Training F1: {max(metrics['eval_f1_scores']):.4f}\")\n",
    "print(f\"  Best Training Return: {max(metrics['eval_returns']):.2f}\")\n",
    "print(f\"  F1 evaluations: {len(metrics['eval_f1_scores'])} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944d9b8e",
   "metadata": {},
   "source": [
    "## Model Evaluation and Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c687d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:49:01.842007Z",
     "iopub.status.busy": "2025-07-29T18:49:01.841679Z",
     "iopub.status.idle": "2025-07-29T18:49:02.916337Z",
     "shell.execute_reply": "2025-07-29T18:49:02.915789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create enhanced visualization with F1 scores\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Critic Loss\n",
    "if 'steps' in metrics and 'critic_losses' in metrics:\n",
    "    axes[0, 0].plot(metrics['steps'], metrics['critic_losses'])\n",
    "    axes[0, 0].set_title('Critic Loss')\n",
    "    axes[0, 0].set_xlabel('Steps')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True)\n",
    "else:\n",
    "    axes[0, 0].text(0.5, 0.5, 'No critic loss data', ha='center', va='center')\n",
    "    axes[0, 0].set_title('Critic Loss')\n",
    "\n",
    "# Actor Loss\n",
    "if 'steps' in metrics and 'actor_losses' in metrics:\n",
    "    axes[0, 1].plot(metrics['steps'], metrics['actor_losses'])\n",
    "    axes[0, 1].set_title('Actor Loss')\n",
    "    axes[0, 1].set_xlabel('Steps')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].grid(True)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'No actor loss data', ha='center', va='center')\n",
    "    axes[0, 1].set_title('Actor Loss')\n",
    "\n",
    "# Alpha (Temperature)\n",
    "if 'steps' in metrics and 'alphas' in metrics:\n",
    "    axes[0, 2].plot(metrics['steps'], metrics['alphas'])\n",
    "    axes[0, 2].set_title('Temperature (Alpha)')\n",
    "    axes[0, 2].set_xlabel('Steps')\n",
    "    axes[0, 2].set_ylabel('Alpha')\n",
    "    axes[0, 2].grid(True)\n",
    "else:\n",
    "    axes[0, 2].text(0.5, 0.5, 'No alpha data', ha='center', va='center')\n",
    "    axes[0, 2].set_title('Temperature (Alpha)')\n",
    "\n",
    "# Evaluation Returns\n",
    "if 'eval_returns' in metrics and metrics['eval_returns']:\n",
    "    axes[1, 0].plot(metrics['eval_steps'], metrics['eval_returns'], 'o-', color='blue', label='Returns')\n",
    "    axes[1, 0].set_title('Evaluation Returns')\n",
    "    axes[1, 0].set_xlabel('Steps')\n",
    "    axes[1, 0].set_ylabel('Return')\n",
    "    axes[1, 0].grid(True)\n",
    "    axes[1, 0].legend()\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No evaluation data', ha='center', va='center')\n",
    "    axes[1, 0].set_title('Evaluation Returns')\n",
    "\n",
    "# F1 Scores During Training\n",
    "if 'eval_f1_scores' in metrics and metrics['eval_f1_scores']:\n",
    "    axes[1, 1].plot(metrics['eval_steps'], metrics['eval_f1_scores'], 'o-', color='green', label='F1 Score')\n",
    "    axes[1, 1].set_title('Training F1 Scores')\n",
    "    axes[1, 1].set_xlabel('Steps')\n",
    "    axes[1, 1].set_ylabel('F1 Score')\n",
    "    axes[1, 1].grid(True)\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # Print F1 progression\n",
    "    print(\"F1 Score Progression During Training:\")\n",
    "    for step, f1 in zip(metrics['eval_steps'], metrics['eval_f1_scores']):\n",
    "        print(f\"  Step {step}: F1 = {f1:.4f}\")\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No F1 score data\\n(Run training first)', ha='center', va='center')\n",
    "    axes[1, 1].set_title('Training F1 Scores')\n",
    "\n",
    "# Musical Metrics Combined\n",
    "if ('eval_precision_scores' in metrics and metrics['eval_precision_scores'] and \n",
    "    'eval_recall_scores' in metrics and metrics['eval_recall_scores']):\n",
    "    \n",
    "    axes[1, 2].plot(metrics['eval_steps'], metrics['eval_precision_scores'], 'o-', color='purple', label='Precision')\n",
    "    axes[1, 2].plot(metrics['eval_steps'], metrics['eval_recall_scores'], 'o-', color='orange', label='Recall')\n",
    "    axes[1, 2].set_title('Precision & Recall')\n",
    "    axes[1, 2].set_xlabel('Steps')\n",
    "    axes[1, 2].set_ylabel('Score')\n",
    "    axes[1, 2].grid(True)\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    # Print musical metrics progression\n",
    "    print(\"\\nMusical Metrics Progression During Training:\")\n",
    "    for step, p, r in zip(metrics['eval_steps'], metrics['eval_precision_scores'], metrics['eval_recall_scores']):\n",
    "        print(f\"  Step {step}: Precision = {p:.4f}, Recall = {r:.4f}\")\n",
    "else:\n",
    "    axes[1, 2].text(0.5, 0.5, 'No musical metrics\\n(Run training first)', ha='center', va='center')\n",
    "    axes[1, 2].set_title('Precision & Recall')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot with timestamp\n",
    "timestamp = int(time.time())\n",
    "plot_filename = results_dir / f\"training_metrics_{timestamp}.png\"\n",
    "plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nPlot saved to: {plot_filename}\")\n",
    "\n",
    "# Also save as a generic filename (latest)\n",
    "latest_plot_filename = results_dir / \"training_metrics_latest.png\"\n",
    "plt.savefig(latest_plot_filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"Plot also saved as: {latest_plot_filename}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Summary of training results\n",
    "if 'eval_f1_scores' in metrics and metrics['eval_f1_scores']:\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"Final F1 Score: {metrics['eval_f1_scores'][-1]:.4f}\")\n",
    "    print(f\"Best F1 Score: {max(metrics['eval_f1_scores']):.4f}\")\n",
    "    print(f\"F1 Score Improvement: {metrics['eval_f1_scores'][-1] - metrics['eval_f1_scores'][0]:+.4f}\")\n",
    "    \n",
    "    if 'eval_returns' in metrics and metrics['eval_returns']:\n",
    "        print(f\"Final Return: {metrics['eval_returns'][-1]:.2f}\")\n",
    "        print(f\"Best Return: {max(metrics['eval_returns']):.2f}\")\n",
    "        print(f\"Return Improvement: {metrics['eval_returns'][-1] - metrics['eval_returns'][0]:+.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pianist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

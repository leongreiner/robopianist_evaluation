{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125d74af",
   "metadata": {},
   "source": [
    "# RoboPianist Training and Evaluation on ROBOPIANIST-REPERTOIRE-150\n",
    "\n",
    "This notebook tests RoboPianist’s ability to reproduce the papers results. We have ported the original kevinzakka/robopianist-rl (https://github.com/kevinzakka/robopianist-rl.git) training pipeline from JAX to PyTorch and adapted the structure of the official tutorial.ipynb (https://github.com/google-research/robopianist/blob/main/tutorial.ipynb) for easy to use and full reproducibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98d4a6",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "1. Follow the [RoboPianist installation instructions](https://github.com/google-research/robopianist?tab=readme-ov-file#installation) to set up the pianist conda environmen.\n",
    "2. Intall the following packages in the pianist conda environment:\n",
    "   ```bash\n",
    "   conda activate pianist\n",
    "   conda install \"numpy<2.0.0\"\n",
    "   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "   conda install -c conda-forge matplotlib tqdm wandb tyro\n",
    "   pip install wandb\n",
    "   ```\n",
    "3. To verify that everything is installed and downloaded, please run once the [tutorial.ipynb](robopianist/tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62a6e3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "451b1443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:38:39.711074Z",
     "iopub.status.busy": "2025-07-29T18:38:39.710691Z",
     "iopub.status.idle": "2025-07-29T18:39:05.197759Z",
     "shell.execute_reply": "2025-07-29T18:39:05.196767Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/0A/di97jur/miniconda3/envs/pianist/lib/python3.10/site-packages/glfw/__init__.py:917: GLFWError: (65550) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n",
      "/dss/dsshome1/0A/di97jur/miniconda3/envs/pianist/lib/python3.10/site-packages/pretty_midi/instrument.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import time\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from tqdm import tqdm\n",
    "import tyro\n",
    "from IPython.display import HTML, clear_output\n",
    "from base64 import b64encode\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_dir = os.getcwd()  \n",
    "robopianist_dir = os.path.join(base_dir, \"robopianist\")\n",
    "os.chdir(robopianist_dir)\n",
    "if robopianist_dir not in sys.path:\n",
    "    sys.path.insert(0, robopianist_dir)\n",
    "\n",
    "# RoboPianist imports\n",
    "from robopianist import suite, music\n",
    "import dm_env_wrappers as wrappers\n",
    "import robopianist.wrappers as robopianist_wrappers\n",
    "import dm_env\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e7e154",
   "metadata": {},
   "source": [
    "## Configuration and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b6e91a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:05.202184Z",
     "iopub.status.busy": "2025-07-29T18:39:05.201783Z",
     "iopub.status.idle": "2025-07-29T18:39:05.211690Z",
     "shell.execute_reply": "2025-07-29T18:39:05.211144Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingArgs:\n",
    "    hpc_mode: bool = True  # Set to False for local runs with video recording\n",
    "    \n",
    "    # Environment settings\n",
    "    environment_name: str = \"RoboPianist-debug-TwinkleTwinkleRousseau-v0\"\n",
    "    seed: int = 42\n",
    "    control_timestep: float = 0.05\n",
    "    n_steps_lookahead: int = 10\n",
    "    trim_silence: bool = True\n",
    "    gravity_compensation: bool = True\n",
    "    reduced_action_space: bool = True\n",
    "    primitive_fingertip_collisions: bool = True\n",
    "    action_reward_observation: bool = True\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    max_steps: int = 5000000\n",
    "    warmstart_steps: int = 5000\n",
    "    batch_size: int = 256\n",
    "    replay_capacity: int = 5000000\n",
    "    \n",
    "    # SAC hyperparameters\n",
    "    actor_lr: float = 3e-4\n",
    "    critic_lr: float = 3e-4\n",
    "    temp_lr: float = 3e-4\n",
    "    hidden_dims: Tuple[int, ...] = (256, 256, 256)\n",
    "    discount: float = 0.99\n",
    "    tau: float = 0.005\n",
    "    init_temperature: float = 1.0\n",
    "    \n",
    "    # Logging and evaluation\n",
    "    log_interval: int = 1000\n",
    "    eval_interval: int = 10000\n",
    "    eval_episodes: int = 1  # Back to 1 to avoid identical episodes (can set to 3+ if needed)\n",
    "    tqdm_bar: bool = True\n",
    "    \n",
    "    # Paths and wandb\n",
    "    base_dir = os.getcwd()\n",
    "    parent_dir = os.path.dirname(base_dir)\n",
    "    root_dir: str = str(os.path.join(parent_dir, \"runs\"))\n",
    "    project: str = \"robopianist-pytorch\"\n",
    "    mode: str = \"disabled\"  # Set to \"online\" to enable wandb logging\n",
    "    \n",
    "    # Device\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create training configuration\n",
    "args = TrainingArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa369495",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:05.214249Z",
     "iopub.status.busy": "2025-07-29T18:39:05.214051Z",
     "iopub.status.idle": "2025-07-29T18:39:05.223252Z",
     "shell.execute_reply": "2025-07-29T18:39:05.222707Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_video(filename: str):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Video file not found: {filename}\")\n",
    "        return None\n",
    "    \n",
    "    mp4 = open(filename, \"rb\").read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    \n",
    "    return HTML(f\"\"\"\n",
    "    <video controls width=\"640\" height=\"480\">\n",
    "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\")\n",
    "\n",
    "def prefix_dict(prefix: str, d: dict) -> dict:\n",
    "    return {f\"{prefix}/{k}\": v for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a593865",
   "metadata": {},
   "source": [
    "## Policy Network in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e403aa79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:05.226601Z",
     "iopub.status.busy": "2025-07-29T18:39:05.226404Z",
     "iopub.status.idle": "2025-07-29T18:39:05.239123Z",
     "shell.execute_reply": "2025-07-29T18:39:05.238602Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        dims = (input_dim,) + hidden_dims + (output_dim,)\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            layers.append(nn.GELU())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.backbone = MLP(obs_dim, 2 * action_dim, hidden_dims)\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        outputs = self.backbone(obs)\n",
    "        means, log_stds = torch.chunk(outputs, 2, dim=-1)\n",
    "        log_stds = torch.clamp(log_stds, -20, 2)\n",
    "        return means, log_stds\n",
    "    \n",
    "    def sample(self, obs, deterministic=False):\n",
    "        means, log_stds = self.forward(obs)\n",
    "        stds = torch.exp(log_stds)\n",
    "        \n",
    "        if deterministic:\n",
    "            actions = torch.tanh(means)\n",
    "            log_probs = None\n",
    "        else:\n",
    "            dist = Normal(means, stds)\n",
    "            samples = dist.rsample()\n",
    "            actions = torch.tanh(samples)\n",
    "            \n",
    "            log_probs = dist.log_prob(samples)\n",
    "            log_probs -= torch.log(1 - actions.pow(2) + 1e-6)\n",
    "            log_probs = log_probs.sum(dim=-1, keepdim=True)\n",
    "            \n",
    "        return actions, log_probs\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.network = MLP(obs_dim + action_dim, 1, hidden_dims)\n",
    "    \n",
    "    def forward(self, obs, action):\n",
    "        x = torch.cat([obs, action], dim=-1)\n",
    "        return self.network(x)\n",
    "\n",
    "class TwinQNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.q1 = QNetwork(obs_dim, action_dim, hidden_dims)\n",
    "        self.q2 = QNetwork(obs_dim, action_dim, hidden_dims)\n",
    "    \n",
    "    def forward(self, obs, action):\n",
    "        return self.q1(obs, action), self.q2(obs, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73fb2d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:05.240777Z",
     "iopub.status.busy": "2025-07-29T18:39:05.240578Z",
     "iopub.status.idle": "2025-07-29T18:39:05.252528Z",
     "shell.execute_reply": "2025-07-29T18:39:05.251791Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, obs_dim: int, action_dim: int, max_size: int, device: str):\n",
    "        self.max_size = max_size\n",
    "        self.device = device\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        self.obs = torch.zeros((max_size, obs_dim), dtype=torch.float32)\n",
    "        self.actions = torch.zeros((max_size, action_dim), dtype=torch.float32)\n",
    "        self.rewards = torch.zeros((max_size, 1), dtype=torch.float32)\n",
    "        self.next_obs = torch.zeros((max_size, obs_dim), dtype=torch.float32)\n",
    "        self.dones = torch.zeros((max_size, 1), dtype=torch.float32)\n",
    "        \n",
    "        self._prev_timestep = None\n",
    "        \n",
    "    def add(self, timestep, action=None):\n",
    "        if action is not None and self._prev_timestep is not None:\n",
    "            obs = torch.from_numpy(self._prev_timestep.observation.astype(np.float32))\n",
    "            next_obs = torch.from_numpy(timestep.observation.astype(np.float32))\n",
    "            action_tensor = torch.from_numpy(action.astype(np.float32))\n",
    "            reward = torch.tensor(float(timestep.reward), dtype=torch.float32).unsqueeze(0)\n",
    "            done = torch.tensor(float(1.0 - timestep.discount), dtype=torch.float32).unsqueeze(0)\n",
    "            \n",
    "            self.obs[self.ptr] = obs\n",
    "            self.actions[self.ptr] = action_tensor\n",
    "            self.rewards[self.ptr] = reward\n",
    "            self.next_obs[self.ptr] = next_obs\n",
    "            self.dones[self.ptr] = done\n",
    "            \n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "            self.size = min(self.size + 1, self.max_size)\n",
    "            \n",
    "        self._prev_timestep = timestep\n",
    "    \n",
    "    def sample(self, batch_size: int):\n",
    "        indices = torch.randint(0, self.size, (batch_size,))\n",
    "        return (\n",
    "            self.obs[indices].to(self.device),\n",
    "            self.actions[indices].to(self.device),\n",
    "            self.rewards[indices].to(self.device),\n",
    "            self.next_obs[indices].to(self.device),\n",
    "            self.dones[indices].to(self.device)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e0a2ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:05.256141Z",
     "iopub.status.busy": "2025-07-29T18:39:05.255947Z",
     "iopub.status.idle": "2025-07-29T18:39:05.270410Z",
     "shell.execute_reply": "2025-07-29T18:39:05.269614Z"
    }
   },
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self, obs_dim: int, action_dim: int, args: TrainingArgs):\n",
    "        self.device = torch.device(args.device)\n",
    "        self.discount = args.discount\n",
    "        self.tau = args.tau\n",
    "        self.target_entropy = -action_dim\n",
    "        \n",
    "        # Networks\n",
    "        self.actor = Policy(obs_dim, action_dim, args.hidden_dims).to(self.device)\n",
    "        self.critic = TwinQNetwork(obs_dim, action_dim, args.hidden_dims).to(self.device)\n",
    "        self.target_critic = TwinQNetwork(obs_dim, action_dim, args.hidden_dims).to(self.device)\n",
    "        self.actor = self.actor.float()\n",
    "        self.critic = self.critic.float()\n",
    "        self.target_critic = self.target_critic.float()\n",
    "        \n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        self.log_alpha = torch.tensor(np.log(args.init_temperature), dtype=torch.float32, requires_grad=True, device=self.device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=args.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=args.critic_lr)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=args.temp_lr)\n",
    "        \n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self.log_alpha.exp()\n",
    "    \n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.from_numpy(obs.astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "            action, _ = self.actor.sample(obs_tensor, deterministic=deterministic)\n",
    "            return action.cpu().numpy()[0].astype(np.float32)\n",
    "    \n",
    "    def update(self, replay_buffer: ReplayBuffer, batch_size: int):\n",
    "        obs, actions, rewards, next_obs, dones = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        obs = obs.float()\n",
    "        actions = actions.float()\n",
    "        rewards = rewards.float()\n",
    "        next_obs = next_obs.float()\n",
    "        dones = dones.float()\n",
    "        \n",
    "        # Update critic\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.actor.sample(next_obs)\n",
    "            target_q1, target_q2 = self.target_critic(next_obs, next_actions)\n",
    "            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs\n",
    "            target_q = rewards + (1 - dones) * torch.tensor(self.discount, dtype=torch.float32, device=self.device) * target_q\n",
    "        \n",
    "        current_q1, current_q2 = self.critic(obs, actions)\n",
    "        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update actor\n",
    "        new_actions, log_probs = self.actor.sample(obs)\n",
    "        q1, q2 = self.critic(obs, new_actions)\n",
    "        q = torch.min(q1, q2)\n",
    "        actor_loss = (self.alpha * log_probs - q).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update temperature\n",
    "        target_entropy_tensor = torch.tensor(self.target_entropy, dtype=torch.float32, device=self.device)\n",
    "        alpha_loss = -(self.log_alpha * (log_probs + target_entropy_tensor).detach()).mean()\n",
    "        \n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.soft_update(self.critic, self.target_critic)\n",
    "        \n",
    "        return {\n",
    "            \"critic_loss\": critic_loss.item(),\n",
    "            \"actor_loss\": actor_loss.item(),\n",
    "            \"alpha_loss\": alpha_loss.item(),\n",
    "            \"alpha\": self.alpha.item(),\n",
    "        }\n",
    "    \n",
    "    def soft_update(self, source, target):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        torch.save({\n",
    "            'actor': self.actor.state_dict(),\n",
    "            'critic': self.critic.state_dict(),\n",
    "            'target_critic': self.target_critic.state_dict(),\n",
    "            'log_alpha': self.log_alpha,\n",
    "        }, filepath)\n",
    "    \n",
    "    def load(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "        self.target_critic.load_state_dict(checkpoint['target_critic'])\n",
    "        self.log_alpha = checkpoint['log_alpha']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82008f",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b790934a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:05.272600Z",
     "iopub.status.busy": "2025-07-29T18:39:05.272402Z",
     "iopub.status.idle": "2025-07-29T18:39:06.914945Z",
     "shell.execute_reply": "2025-07-29T18:39:06.914006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test environment...\n",
      "Observation dimension: 1164\n",
      "Action dimension: 39\n"
     ]
    }
   ],
   "source": [
    "def get_env(args: TrainingArgs, record_dir: Optional[Path] = None):\n",
    "    env = suite.load(\n",
    "        environment_name=args.environment_name,\n",
    "        seed=args.seed,\n",
    "        task_kwargs=dict(\n",
    "            n_steps_lookahead=args.n_steps_lookahead,\n",
    "            trim_silence=args.trim_silence,\n",
    "            gravity_compensation=args.gravity_compensation,\n",
    "            reduced_action_space=args.reduced_action_space,\n",
    "            control_timestep=args.control_timestep,\n",
    "            primitive_fingertip_collisions=args.primitive_fingertip_collisions,\n",
    "            change_color_on_activation=True,\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    if record_dir is not None and not args.hpc_mode:\n",
    "        env = robopianist_wrappers.PianoSoundVideoWrapper(\n",
    "            environment=env,\n",
    "            record_dir=record_dir,\n",
    "            record_every=1,\n",
    "            camera_id=\"piano/back\",\n",
    "            height=480,\n",
    "            width=640,\n",
    "        )\n",
    "    \n",
    "    env = wrappers.EpisodeStatisticsWrapper(environment=env, deque_size=1)\n",
    "    env = robopianist_wrappers.MidiEvaluationWrapper(environment=env, deque_size=1)\n",
    "    \n",
    "    if args.action_reward_observation:\n",
    "        env = wrappers.ObservationActionRewardWrapper(env)\n",
    "    \n",
    "    env = wrappers.ConcatObservationWrapper(env)\n",
    "    env = wrappers.CanonicalSpecWrapper(env, clip=True)\n",
    "    env = wrappers.SinglePrecisionWrapper(env)\n",
    "    env = wrappers.DmControlWrapper(env)\n",
    "    \n",
    "    return env\n",
    "\n",
    "# Test environment creation\n",
    "print(\"Creating test environment...\")\n",
    "test_env = get_env(args)\n",
    "test_timestep = test_env.reset()\n",
    "\n",
    "obs_dim = test_timestep.observation.shape[0]\n",
    "action_dim = test_env.action_spec().shape[0]\n",
    "\n",
    "print(f\"Observation dimension: {obs_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198fff8e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06aa9384",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:06.917299Z",
     "iopub.status.busy": "2025-07-29T18:39:06.917075Z",
     "iopub.status.idle": "2025-07-29T18:39:06.937991Z",
     "shell.execute_reply": "2025-07-29T18:39:06.937178Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(args: TrainingArgs):\n",
    "    \"\"\"Train the SAC agent.\"\"\"\n",
    "    \n",
    "    # Set random seeds\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    print(f\"Using device: {args.device}\")\n",
    "    print(f\"HPC Mode: {'ENABLED' if args.hpc_mode else 'DISABLED'}\")\n",
    "    if args.hpc_mode:\n",
    "        print(\"  - Video recording disabled\")\n",
    "    \n",
    "    # Create experiment directory\n",
    "    run_name = f\"PyTorch-SAC-{args.environment_name}-{args.seed}-{int(time.time())}\"\n",
    "    experiment_dir = Path(args.root_dir) / run_name\n",
    "    experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Experiment directory: {experiment_dir}\")\n",
    "    \n",
    "    # Initialize wandb\n",
    "    if args.mode != \"disabled\":\n",
    "        wandb.init(\n",
    "            project=args.project,\n",
    "            config=asdict(args),\n",
    "            mode=args.mode,\n",
    "            name=run_name,\n",
    "        )\n",
    "    \n",
    "    # Create environments\n",
    "    env = get_env(args)\n",
    "    eval_env = get_env(args, record_dir=experiment_dir / \"eval\")\n",
    "    \n",
    "    # Initialize SAC agent\n",
    "    agent = SAC(obs_dim, action_dim, args)\n",
    "    \n",
    "    # Initialize replay buffer\n",
    "    replay_buffer = ReplayBuffer(obs_dim, action_dim, args.replay_capacity, args.device)\n",
    "    \n",
    "    # Training metrics\n",
    "    training_metrics = {\n",
    "        'steps': [],\n",
    "        'critic_losses': [],\n",
    "        'actor_losses': [],\n",
    "        'alpha_losses': [],\n",
    "        'alphas': [],\n",
    "        'eval_returns': [],\n",
    "        'eval_f1_scores': [], \n",
    "        'eval_precision_scores': [], \n",
    "        'eval_recall_scores': [], \n",
    "        'eval_steps': []\n",
    "    }\n",
    "    \n",
    "    # Add initial timestep\n",
    "    timestep = env.reset()\n",
    "    replay_buffer.add(timestep)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    eval_count = 0 \n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"Starting training loop for {args.max_steps:,} steps...\")\n",
    "    progress_bar = tqdm(range(1, args.max_steps + 1), desc=\"Training\", unit=\"steps\")\n",
    "    \n",
    "    for step in progress_bar:\n",
    "        # Select action\n",
    "        if step < args.warmstart_steps:\n",
    "            action = env.action_spec().generate_value()\n",
    "        else:\n",
    "            action = agent.select_action(timestep.observation)\n",
    "        \n",
    "        # Step environment\n",
    "        timestep = env.step(action)\n",
    "        replay_buffer.add(timestep, action)\n",
    "        \n",
    "        # Reset if episode ended\n",
    "        if timestep.last():\n",
    "            if args.mode != \"disabled\":\n",
    "                wandb.log(prefix_dict(\"train\", env.get_statistics()), step=step)\n",
    "            timestep = env.reset()\n",
    "            replay_buffer.add(timestep)\n",
    "        \n",
    "        # Update agent\n",
    "        if step >= args.warmstart_steps and len(replay_buffer) >= args.batch_size:\n",
    "            metrics = agent.update(replay_buffer, args.batch_size)\n",
    "            \n",
    "            # Store and update progress bar\n",
    "            if step % args.log_interval == 0:\n",
    "                training_metrics['steps'].append(step)\n",
    "                training_metrics['critic_losses'].append(metrics['critic_loss'])\n",
    "                training_metrics['actor_losses'].append(metrics['actor_loss'])\n",
    "                training_metrics['alpha_losses'].append(metrics['alpha_loss'])\n",
    "                training_metrics['alphas'].append(metrics['alpha'])\n",
    "                \n",
    "                # Update progress bar description with latest metrics\n",
    "                progress_bar.set_postfix({\n",
    "                    'Critic': f\"{metrics['critic_loss']:.3f}\",\n",
    "                    'Actor': f\"{metrics['actor_loss']:.3f}\",\n",
    "                    'Alpha': f\"{metrics['alpha']:.3f}\",\n",
    "                    'Buffer': f\"{len(replay_buffer)}\"\n",
    "                })\n",
    "                \n",
    "                if args.mode != \"disabled\":\n",
    "                    wandb.log(prefix_dict(\"train\", metrics), step=step)\n",
    "        \n",
    "        if step % args.eval_interval == 0:\n",
    "            eval_count += 1 \n",
    "            \n",
    "            eval_returns = []\n",
    "            eval_f1s = []\n",
    "            eval_precisions = []\n",
    "            eval_recalls = []\n",
    "            \n",
    "            actual_eval_episodes = args.eval_episodes\n",
    "            \n",
    "            for episode in range(actual_eval_episodes):\n",
    "                episode_eval_env = get_env(args, record_dir=experiment_dir / \"eval\")\n",
    "                eval_timestep = episode_eval_env.reset()\n",
    "                eval_return = 0.0\n",
    "                \n",
    "                use_deterministic = (episode == 0) if actual_eval_episodes > 1 else True\n",
    "                \n",
    "                while not eval_timestep.last():\n",
    "                    eval_action = agent.select_action(eval_timestep.observation, deterministic=use_deterministic)\n",
    "                    eval_timestep = episode_eval_env.step(eval_action)\n",
    "                    eval_return += eval_timestep.reward\n",
    "                \n",
    "                eval_returns.append(eval_return)\n",
    "                \n",
    "                try:\n",
    "                    musical_metrics = episode_eval_env.get_musical_metrics()\n",
    "                    eval_f1s.append(musical_metrics['f1'])\n",
    "                    eval_precisions.append(musical_metrics['precision'])\n",
    "                    eval_recalls.append(musical_metrics['recall'])\n",
    "                except (AttributeError, ValueError):\n",
    "                    eval_f1s.append(0.0)\n",
    "                    eval_precisions.append(0.0)\n",
    "                    eval_recalls.append(0.0)\n",
    "            \n",
    "            mean_return = np.mean(eval_returns)\n",
    "            mean_f1 = np.mean(eval_f1s)\n",
    "            mean_precision = np.mean(eval_precisions)\n",
    "            mean_recall = np.mean(eval_recalls)\n",
    "            \n",
    "            # Calculate standard deviations\n",
    "            std_return = np.std(eval_returns) if len(eval_returns) > 1 else 0.0\n",
    "            std_f1 = np.std(eval_f1s) if len(eval_f1s) > 1 else 0.0\n",
    "            std_precision = np.std(eval_precisions) if len(eval_precisions) > 1 else 0.0\n",
    "            std_recall = np.std(eval_recalls) if len(eval_recalls) > 1 else 0.0\n",
    "            \n",
    "            # Store metrics\n",
    "            training_metrics['eval_returns'].append(mean_return)\n",
    "            training_metrics['eval_f1_scores'].append(mean_f1)\n",
    "            training_metrics['eval_precision_scores'].append(mean_precision)\n",
    "            training_metrics['eval_recall_scores'].append(mean_recall)\n",
    "            training_metrics['eval_steps'].append(step)\n",
    "            \n",
    "            if args.mode != \"disabled\":\n",
    "                eval_metrics = {\n",
    "                    \"eval/return_mean\": mean_return,\n",
    "                    \"eval/return_std\": std_return,\n",
    "                    \"eval/f1_mean\": mean_f1,\n",
    "                    \"eval/f1_std\": std_f1,\n",
    "                    \"eval/precision_mean\": mean_precision,\n",
    "                    \"eval/precision_std\": std_precision,\n",
    "                    \"eval/recall_mean\": mean_recall,\n",
    "                    \"eval/recall_std\": std_recall,\n",
    "                }\n",
    "                eval_metrics.update(prefix_dict(\"eval\", eval_env.get_statistics()))\n",
    "                wandb.log(eval_metrics, step=step)\n",
    "            \n",
    "            eval_info = f\"Eval #{eval_count}: R={mean_return:.1f}, F1={mean_f1:.4f}\"\n",
    "            if std_return > 0:\n",
    "                eval_info += f\"±{std_return:.1f}\"\n",
    "            if std_f1 > 0:\n",
    "                eval_info += f\"±{std_f1:.4f}\"\n",
    "                \n",
    "            progress_bar.set_postfix({\n",
    "                'Last_Eval': eval_info,\n",
    "                'Buffer': f\"{len(replay_buffer)}\"\n",
    "            })\n",
    "            \n",
    "            # Save model at every evaluation step in HPC mode\n",
    "            if args.hpc_mode:\n",
    "                model_path = experiment_dir / f\"model_step_{step}_eval_{eval_count}.pt\"\n",
    "                agent.save(model_path)\n",
    "                print(f\"Model saved at step {step}: {model_path}\")\n",
    "    \n",
    "    progress_bar.close()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Total evaluations performed: {eval_count}\")\n",
    "    \n",
    "    # Save final model\n",
    "    model_path = experiment_dir / \"final_model.pt\"\n",
    "    agent.save(model_path)\n",
    "    print(f\"Final model saved to: {model_path}\")\n",
    "    \n",
    "    if args.hpc_mode:\n",
    "        print(f\"HPC Mode: {eval_count} evaluation models saved in {experiment_dir}\")\n",
    "        print(\"To generate videos locally later, set hpc_mode=False and reload saved models\")\n",
    "    \n",
    "    if args.mode != \"disabled\":\n",
    "        wandb.finish()\n",
    "    \n",
    "    return agent, training_metrics, experiment_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "537cf2b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:39:06.940074Z",
     "iopub.status.busy": "2025-07-29T18:39:06.939868Z",
     "iopub.status.idle": "2025-07-29T18:49:01.837184Z",
     "shell.execute_reply": "2025-07-29T18:49:01.836603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5,000,000 steps with 5,000 warmstart steps\n",
      "Batch size: 256, Replay capacity: 5,000,000\n",
      "Evaluation interval: 10,000 steps\n",
      "Evaluation episodes per interval: 1\n",
      "Using device: cpu\n",
      "HPC Mode: ENABLED\n",
      "  - Video recording disabled\n",
      "Experiment directory: /dss/dsstbyfs02/pn52ru/pn52ru-dss-0000/di97jur/robopianist_evaluation/runs/PyTorch-SAC-RoboPianist-debug-TwinkleTwinkleRousseau-v0-42-1753819072\n",
      "Starting training loop for 5,000,000 steps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 4172/5000000 [00:36<12:01:30, 115.40steps/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation episodes per interval: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39meval_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run enhanced training with clean output\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m agent, metrics, exp_dir \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperiment directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 74\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     71\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(timestep\u001b[38;5;241m.\u001b[39mobservation)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Step environment\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m timestep \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m replay_buffer\u001b[38;5;241m.\u001b[39madd(timestep, action)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Reset if episode ended\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pianist/lib/python3.10/site-packages/dm_env_wrappers/_src/base.py:52\u001b[0m, in \u001b[0;36mEnvironmentWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m dm_env\u001b[38;5;241m.\u001b[39mTimeStep:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pianist/lib/python3.10/site-packages/dm_env_wrappers/_src/single_precision.py:35\u001b[0m, in \u001b[0;36mSinglePrecisionWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m dm_env\u001b[38;5;241m.\u001b[39mTimeStep:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_timestep(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pianist/lib/python3.10/site-packages/dm_env_wrappers/_src/canonical_spec.py:50\u001b[0m, in \u001b[0;36mCanonicalSpecWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m dm_env\u001b[38;5;241m.\u001b[39mTimeStep:\n\u001b[1;32m     49\u001b[0m     scaled_action \u001b[38;5;241m=\u001b[39m _scale_nested_action(action, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_action_spec, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip)\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_action\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pianist/lib/python3.10/site-packages/dm_env_wrappers/_src/concatenate_observations.py:90\u001b[0m, in \u001b[0;36mConcatObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m dm_env\u001b[38;5;241m.\u001b[39mTimeStep:\n\u001b[0;32m---> 90\u001b[0m     timestep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m timestep\u001b[38;5;241m.\u001b[39m_replace(\n\u001b[1;32m     92\u001b[0m         observation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_observation(timestep\u001b[38;5;241m.\u001b[39mobservation)\n\u001b[1;32m     93\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pianist/lib/python3.10/site-packages/dm_env_wrappers/_src/observation_action_reward.py:39\u001b[0m, in \u001b[0;36mObservationActionRewardWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m dm_env\u001b[38;5;241m.\u001b[39mTimeStep:\n\u001b[0;32m---> 39\u001b[0m     timestep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_augment_observation(action, timestep\u001b[38;5;241m.\u001b[39mreward, timestep)\n",
      "File \u001b[0;32m/dss/dsstbyfs02/pn52ru/pn52ru-dss-0000/di97jur/robopianist_evaluation/robopianist/robopianist/wrappers/evaluation.py:68\u001b[0m, in \u001b[0;36mMidiEvaluationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m dm_env\u001b[38;5;241m.\u001b[39mTimeStep:\n\u001b[0;32m---> 68\u001b[0m     timestep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     key_activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_environment\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39mpiano\u001b[38;5;241m.\u001b[39mactivation\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_presses\u001b[38;5;241m.\u001b[39mappend(key_activation\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64))\n",
      "File \u001b[0;32m~/miniconda3/envs/pianist/lib/python3.10/site-packages/dm_env_wrappers/_src/episode_statistics.py:36\u001b[0m, in \u001b[0;36mEpisodeStatisticsWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m dm_env\u001b[38;5;241m.\u001b[39mTimeStep:\n\u001b[0;32m---> 36\u001b[0m     timestep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_episode_return \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m timestep\u001b[38;5;241m.\u001b[39mreward\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_episode_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pianist/lib/python3.10/site-packages/dm_control/composer/environment.py:423\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sub_steps):\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_substep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;66;03m# The final observation update must happen after all the hooks in\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# `self._hooks.after_step` is called. Otherwise, if any of these hooks\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;66;03m# modify the physics state then we might capture an observation that is\u001b[39;00m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;66;03m# inconsistent with the final physics state.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sub_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pianist/lib/python3.10/site-packages/dm_control/composer/environment.py:464\u001b[0m, in \u001b[0;36mEnvironment._substep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_substep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    462\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks\u001b[38;5;241m.\u001b[39mbefore_substep(\n\u001b[1;32m    463\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physics_proxy, action, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_state)\n\u001b[0;32m--> 464\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_physics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks\u001b[38;5;241m.\u001b[39mafter_substep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_physics_proxy, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/pianist/lib/python3.10/site-packages/dm_control/mujoco/engine.py:174\u001b[0m, in \u001b[0;36mPhysics.step\u001b[0;34m(self, nstep)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_invalid_state():\n\u001b[1;32m    173\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy_step:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_with_up_to_date_position_velocity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     mujoco\u001b[38;5;241m.\u001b[39mmj_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mptr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mptr, nstep)\n",
      "File \u001b[0;32m~/miniconda3/envs/pianist/lib/python3.10/site-packages/dm_control/mujoco/engine.py:162\u001b[0m, in \u001b[0;36mPhysics._step_with_up_to_date_position_velocity\u001b[0;34m(self, nstep)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m   mujoco\u001b[38;5;241m.\u001b[39mmj_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mptr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mptr, nstep)\n\u001b[0;32m--> 162\u001b[0m \u001b[43mmujoco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmj_step1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mptr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Training for {args.max_steps:,} steps with {args.warmstart_steps:,} warmstart steps\")\n",
    "print(f\"Batch size: {args.batch_size}, Replay capacity: {args.replay_capacity:,}\")\n",
    "print(f\"Evaluation interval: {args.eval_interval:,} steps\")\n",
    "print(f\"Evaluation episodes per interval: {args.eval_episodes}\")\n",
    "\n",
    "# Run enhanced training with clean output\n",
    "agent, metrics, exp_dir = train(args)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Experiment directory: {exp_dir}\")\n",
    "if args.hpc_mode:\n",
    "    print(\"HPC Mode was enabled - videos were not recorded to avoid rendering issues\")\n",
    "    print(\"Models were saved at every evaluation step for later video generation\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Final Training F1: {metrics['eval_f1_scores'][-1]:.4f}\")\n",
    "print(f\"  Final Training Return: {metrics['eval_returns'][-1]:.2f}\")\n",
    "print(f\"  Best Training F1: {max(metrics['eval_f1_scores']):.4f}\")\n",
    "print(f\"  Best Training Return: {max(metrics['eval_returns']):.2f}\")\n",
    "print(f\"  F1 evaluations: {len(metrics['eval_f1_scores'])} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944d9b8e",
   "metadata": {},
   "source": [
    "## Model Evaluation and Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c687d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T18:49:01.842007Z",
     "iopub.status.busy": "2025-07-29T18:49:01.841679Z",
     "iopub.status.idle": "2025-07-29T18:49:02.916337Z",
     "shell.execute_reply": "2025-07-29T18:49:02.915789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create enhanced visualization with F1 scores\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Critic Loss\n",
    "if 'steps' in metrics and 'critic_losses' in metrics:\n",
    "    axes[0, 0].plot(metrics['steps'], metrics['critic_losses'])\n",
    "    axes[0, 0].set_title('Critic Loss')\n",
    "    axes[0, 0].set_xlabel('Steps')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True)\n",
    "else:\n",
    "    axes[0, 0].text(0.5, 0.5, 'No critic loss data', ha='center', va='center')\n",
    "    axes[0, 0].set_title('Critic Loss')\n",
    "\n",
    "# Actor Loss\n",
    "if 'steps' in metrics and 'actor_losses' in metrics:\n",
    "    axes[0, 1].plot(metrics['steps'], metrics['actor_losses'])\n",
    "    axes[0, 1].set_title('Actor Loss')\n",
    "    axes[0, 1].set_xlabel('Steps')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].grid(True)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'No actor loss data', ha='center', va='center')\n",
    "    axes[0, 1].set_title('Actor Loss')\n",
    "\n",
    "# Alpha (Temperature)\n",
    "if 'steps' in metrics and 'alphas' in metrics:\n",
    "    axes[0, 2].plot(metrics['steps'], metrics['alphas'])\n",
    "    axes[0, 2].set_title('Temperature (Alpha)')\n",
    "    axes[0, 2].set_xlabel('Steps')\n",
    "    axes[0, 2].set_ylabel('Alpha')\n",
    "    axes[0, 2].grid(True)\n",
    "else:\n",
    "    axes[0, 2].text(0.5, 0.5, 'No alpha data', ha='center', va='center')\n",
    "    axes[0, 2].set_title('Temperature (Alpha)')\n",
    "\n",
    "# Evaluation Returns\n",
    "if 'eval_returns' in metrics and metrics['eval_returns']:\n",
    "    axes[1, 0].plot(metrics['eval_steps'], metrics['eval_returns'], 'o-', color='blue', label='Returns')\n",
    "    axes[1, 0].set_title('Evaluation Returns')\n",
    "    axes[1, 0].set_xlabel('Steps')\n",
    "    axes[1, 0].set_ylabel('Return')\n",
    "    axes[1, 0].grid(True)\n",
    "    axes[1, 0].legend()\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No evaluation data', ha='center', va='center')\n",
    "    axes[1, 0].set_title('Evaluation Returns')\n",
    "\n",
    "# F1 Scores During Training\n",
    "if 'eval_f1_scores' in metrics and metrics['eval_f1_scores']:\n",
    "    axes[1, 1].plot(metrics['eval_steps'], metrics['eval_f1_scores'], 'o-', color='green', label='F1 Score')\n",
    "    axes[1, 1].set_title('Training F1 Scores')\n",
    "    axes[1, 1].set_xlabel('Steps')\n",
    "    axes[1, 1].set_ylabel('F1 Score')\n",
    "    axes[1, 1].grid(True)\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # Print F1 progression\n",
    "    print(\"F1 Score Progression During Training:\")\n",
    "    for step, f1 in zip(metrics['eval_steps'], metrics['eval_f1_scores']):\n",
    "        print(f\"  Step {step}: F1 = {f1:.4f}\")\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No F1 score data\\n(Run training first)', ha='center', va='center')\n",
    "    axes[1, 1].set_title('Training F1 Scores')\n",
    "\n",
    "# Musical Metrics Combined\n",
    "if ('eval_precision_scores' in metrics and metrics['eval_precision_scores'] and \n",
    "    'eval_recall_scores' in metrics and metrics['eval_recall_scores']):\n",
    "    \n",
    "    axes[1, 2].plot(metrics['eval_steps'], metrics['eval_precision_scores'], 'o-', color='purple', label='Precision')\n",
    "    axes[1, 2].plot(metrics['eval_steps'], metrics['eval_recall_scores'], 'o-', color='orange', label='Recall')\n",
    "    axes[1, 2].set_title('Precision & Recall')\n",
    "    axes[1, 2].set_xlabel('Steps')\n",
    "    axes[1, 2].set_ylabel('Score')\n",
    "    axes[1, 2].grid(True)\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    # Print musical metrics progression\n",
    "    print(\"\\nMusical Metrics Progression During Training:\")\n",
    "    for step, p, r in zip(metrics['eval_steps'], metrics['eval_precision_scores'], metrics['eval_recall_scores']):\n",
    "        print(f\"  Step {step}: Precision = {p:.4f}, Recall = {r:.4f}\")\n",
    "else:\n",
    "    axes[1, 2].text(0.5, 0.5, 'No musical metrics\\n(Run training first)', ha='center', va='center')\n",
    "    axes[1, 2].set_title('Precision & Recall')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of training results\n",
    "if 'eval_f1_scores' in metrics and metrics['eval_f1_scores']:\n",
    "    print(f\"\\nTraining Summary:\")\n",
    "    print(f\"Final F1 Score: {metrics['eval_f1_scores'][-1]:.4f}\")\n",
    "    print(f\"Best F1 Score: {max(metrics['eval_f1_scores']):.4f}\")\n",
    "    print(f\"F1 Score Improvement: {metrics['eval_f1_scores'][-1] - metrics['eval_f1_scores'][0]:+.4f}\")\n",
    "    \n",
    "    if 'eval_returns' in metrics and metrics['eval_returns']:\n",
    "        print(f\"Final Return: {metrics['eval_returns'][-1]:.2f}\")\n",
    "        print(f\"Best Return: {max(metrics['eval_returns']):.2f}\")\n",
    "        print(f\"Return Improvement: {metrics['eval_returns'][-1] - metrics['eval_returns'][0]:+.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pianist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
